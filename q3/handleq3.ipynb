{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import itertools\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "\n",
    "#filter(None, re.split('(-|:|am|pm)', '30'))\n",
    "\n",
    "\n",
    "#loading word embeddings\n",
    "#wv = KeyedVectors.load_word2vec_format(\"../w2v/w2vemb.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(n_clusters_, labels, corpus, mapping_dict, test_corpus):\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        # print indice_cluster\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        f1 = open('cluster_%s/%dth_cluster.txt' % (question_class, indice_cluster), 'w+')\n",
    "        for idx in idx_list:\n",
    "            f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "    f1.close()\n",
    "\n",
    "\n",
    "def write_ne(nes, question_class, mapping_dict):\n",
    "    f1 = open('ne_%s.txt' % question_class, 'w+')\n",
    "    for idx, doc in enumerate(nes):\n",
    "        f1.write('%s %s\\n' % (mapping_dict[idx], ' '.join(doc)))\n",
    "    f1.close()    \n",
    "\n",
    "def wmd_compute(x1, x2):\n",
    "    return wv.wmdistance(x1, x2)\n",
    "\n",
    "def analyze_cluster(n_clusters_, labels, corpus, mapping_dict, test_corpus, filename):\n",
    "    num = 0\n",
    "    other_content = []\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        if len(idx_list) > 4:\n",
    "            f1 = open('%s_%s/%dth_cluster.txt' % (filename, question_class, indice_cluster), 'w+')\n",
    "            for idx in idx_list:\n",
    "                f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "            f1.close()\n",
    "        if len(idx_list) < 4:\n",
    "            num = num + len(idx_list)\n",
    "            for idx in idx_list:\n",
    "                other_content.append(corpus[mapping_dict[idx] - 1])\n",
    "    f1 = open('%s_%s/other_cluster.txt' % (filename, question_class), 'w+')\n",
    "    for doc in other_content:\n",
    "        f1.write('%s\\n' % doc)\n",
    "    f1.close()\n",
    "    print num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of comments 478\n"
     ]
    }
   ],
   "source": [
    "question_class = 'q2'\n",
    "fname = '..//dataset//%s.txt' % question_class\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "print 'length of comments', len(content)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_corpus(x1):\n",
    "    x_new = []\n",
    "    for x in x1:\n",
    "        if 'no improvement' in x:\n",
    "            continue\n",
    "        else:\n",
    "            x_new.append(x)\n",
    "    return x_new\n",
    "\n",
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "def process_corpus(x1, day_senswords, time_senswords, general_stop):\n",
    "    # split three categories: 1 no improvemnt 2 with noun 3 others\n",
    "    doc_noimprove = []\n",
    "    doc_days = []\n",
    "    doc_time = []\n",
    "    doc_other = []\n",
    "    doc_multi = []\n",
    "    doc_nn = []\n",
    "    for x in x1:\n",
    "        if 'no improvement' in x or 'respondent' in x:\n",
    "            doc_noimprove.append(x)\n",
    "        else:\n",
    "            sents = sent_tokenize(x)\n",
    "            if len(sents) > 1:\n",
    "                doc_multi.append(x)\n",
    "            else:\n",
    "                nn_list = []\n",
    "                sen = sents[0]\n",
    "                word_list = nltk.word_tokenize(sen)\n",
    "                if not set(word_list).isdisjoint(day_senswords):\n",
    "                    doc_days.append(sen)\n",
    "                else:\n",
    "                    clean_word_list = []\n",
    "                    for word in word_list:\n",
    "                        clean_word_list = clean_word_list + filter(None, re.split('(-|:|am|pm)', word)) \n",
    "                    if 'am' in clean_word_list or 'pm' in clean_word_list or num_there(clean_word_list):\n",
    "                        doc_time.append(sen)\n",
    "                        doc_nn.append(clean_word_list)\n",
    "                    else:\n",
    "                        doc_other.append(sen)\n",
    "\n",
    "    return doc_days, [doc_time,doc_nn], doc_other\n",
    "    #return doc_noimprove, [doc_nn, nn_extracted], doc_other, doc_multi\n",
    "        \n",
    "nn_corpus = []\n",
    "\n",
    "# English stop words lists\n",
    "stop_words = stopwords.words('english')\n",
    "punctuation_list = [unicode(i) for i in string.punctuation]\n",
    "\n",
    "for punctuation in punctuation_list:\n",
    "    stop_words.append(punctuation)\n",
    "\n",
    "day_sense = ['sunday', 'sundays', 'weekend', 'weekends', 'holidays', 'holiday']\n",
    "time_sense = ['am', 'pm',]\n",
    "t1, t2, t3 = process_corpus(content, day_sense, time_sense, stop_words)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "['service', 'center', 'should', 'be', 'open', 'till', '7', 'pm', '.']\n",
      "7\n",
      "['if', 'a', 'customer', 'gives', 'car', 'for', 'servicing', 'then', 'they', 'should', 'do', 'service', 'properly', 'and', 'extend', 'the', 'dealership', 'evening', 'closing', 'time', 'after', '6', 'pm', '.']\n",
      "6\n",
      "None\n",
      "['they', 'open', 'by', '10', 'am', 'in', 'the', 'morning', 'instead', 'they', 'should', 'open', 'by', '9', 'am', 'and', 'close', 'by', '7', 'pm', '.']\n",
      "7\n",
      "None\n",
      "None\n",
      "['the', 'closing', 'time', 'of', 'dealership', 'should', 'increase', 'after', '7', '00', 'pm']\n",
      "7.0\n",
      "None\n",
      "None\n",
      "None\n",
      "['in', 'the', 'morning', 'dealership', 'should', 'open', 'by', '8', 'am', 'and', 'by', '5', 'pm', 'it', 'should', 'be', 'closed', '.']\n",
      "5\n",
      "None\n",
      "['center', 'should', 'be', 'open', 'at', '9', 'am', 'and', 'close', 'by', '7', 'pm', '.']\n",
      "7\n",
      "None\n",
      "['dealership', 'should', 'keep', 'open', 'till', '10', 'pm', '.']\n",
      "10\n",
      "None\n",
      "None\n",
      "['dealership', 'opening', 'closing', 'time', 'should', 'keep', 'the', 'opening', 'till', '8', 'pm']\n",
      "8\n",
      "None\n",
      "['they', 'closed', 'their', 'showroom', 'at', '6', '00', 'pm', ',', 'so', 'if', 'they', 'it', 'as', 'closing', 'time', 'at', '7', '00', 'pm', 'then', 'it', 'would', 'be', 'better', 'for', 'working', 'customers', '.']\n",
      "7.0\n",
      "['nippon', 'toyota', 'service', 'center', 'timing', 'should', 'be', 'open', 'at', 'morning', '7', '00', 'am', 'also', 'close', 'evening', '7', '00', 'pm', '.']\n",
      "7.0\n",
      "None\n",
      "['service', 'center', 'should', 'be', 'opened', 'at', '9', '00', 'am', 'and', 'it', 'gets', 'close', 'at', '5', '00', 'pm', 'instead', 'of', 'that', 'it', 'should', 'be', 'closed', 'by', '6', '00', 'pm', '.']\n",
      "6.0\n",
      "None\n",
      "None\n",
      "['service', 'center', 'should', 'be', 'open', 'at', 'evening', 'from', '6', 'pm', '.']\n",
      "6\n",
      "None\n",
      "None\n",
      "['they', 'have', 'to', 'increase', 'the', 'closing', 'time', 'unto', '7', '30', 'pm', '.']\n",
      "7.5\n",
      "None\n",
      "None\n",
      "['timing', 'should', 'be', 'from', '9', '00', 'am', 'to', '6', '00', 'pm', 'there', 'should', 'be', '1', 'hour', 'lunch', 'break', '.']\n",
      "6.0\n",
      "None\n",
      "['opening', 'timing', 'of', 'service', 'center', 'is', 'from', '9', '30', 'to', '5', '00', 'pm', 'but', 'if', 'the', 'opening', 'time', 'will', 'be', '8', 'am', 'and', 'closing', 'time', 'will', 'be', '9', '30', 'pm', 'then', 'it', 'would', 'be', 'better']\n",
      "9.5\n",
      "['if', 'the', 'opening', 'will', 'be', '10', 'pm', 'and', 'closing', 'time', 'will', 'be', '9', 'pm', 'then', 'it', 'would', 'be', 'better', '.']\n",
      "10\n",
      "['service', 'timing', 'should', 'be', 'from', 'morning', '8', '30', 'to', 'evening', '6', '00', 'pm', '.']\n",
      "6.0\n",
      "['selling', 'or', 'servicing', 'time', 'should', 'be', 's', 'am', 'e', 'in', 'working', 'days', 'monday', 'to', 'friday', 'shop', 'should', 'be', 'open', 'till', 'evening', '7', '30', 'pm']\n",
      "7.5\n",
      "None\n",
      "['they', 'should', 'increase', 'the', 'working', 'timing', 'of', 'dealership', ',', 'they', 'should', 'work', 'till', '8', 'pm', 'instead', 'of', '6', 'pm']\n",
      "8\n",
      "['they', 'should', 'improve', 'the', 'opening', 'and', 'closing', 'time', 'if', 'the', 'opening', 'time', 'will', 'be', '8', 'am', 'and', 'closing', 'time', 'will', 'be', '10', 'pm', 'then', 'it', 'would', 'be', 'good', '.']\n",
      "10\n",
      "['the', 'opening', 'and', 'closing', 'time', 'of', 'dealership', 'should', 'be', '8', 'am', 'to', '6', 'pm', '.']\n",
      "6\n",
      "None\n",
      "None\n",
      "None\n",
      "['jmk', 'toyota', 'showroom', 'timing', 'should', 'be', 'from', '9', '00', 'am', 'to', '5', '00', 'pm', '.']\n",
      "5.0\n",
      "['they', 'should', 'opened', 'dealership', 'at', '10', 'pm', 'but', 'they', 'should', 'open', 'it', 'on', '8', \"o'clock\", 'morning', '.']\n",
      "10\n",
      "['dealership', 'closed', 'at', '5', 'pm', ',', 'it', 'must', 'be', 'there', 'at', '7', 'pm', '.']\n",
      "7\n",
      "None\n",
      "None\n",
      "['the', 'closing', 'time', 'of', 'dealership', 'should', 'be', 'increased', 'there', 'should', 'be', '2', 'shift', 'available', 'till', '8', '30', 'pm', 'then', 'it', 'will', 'be', 'fine', '.']\n",
      "8.5\n",
      "None\n",
      "None\n",
      "None\n",
      "['the', 'service', 'center', 'should', 'be', 'open', 'till', '7', '00', 'pm']\n",
      "7.0\n",
      "None\n",
      "['dealership', 'should', 'be', 'open', 'till', 'at', '8', '30', 'am', 'and', 'close', 'till', 'at', '7', '00', 'pm', '.']\n",
      "7.0\n",
      "['the', 'closing', 'time', 'of', 'harsh', 'thiruvallur', 'dealership', 'was', '6', '00', 'pm', 'but', 'it', 'should', 'be', 'extended', 'and', 'close', 'at', '7', '00', 'pm', 'then', 'it', 'will', 'be', 'good']\n",
      "7.0\n",
      "['the', 'close', 'the', 'service', 'center', 'at', '6', '00', 'pm', 'but', 'they', 'should', 'close', 'the', 'center', 'at', '8', '00', 'to', '9', '00', 'pm', 'then', 'it', 'will', 'be', 'good', '.']\n",
      "9.0\n",
      "['dealership', 'should', 'be', 'available', 'till', '9', '00', 'pm', '.']\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "def extract_am(word_list):\n",
    "    if 'am' in word_list:\n",
    "        amidxs = [i for i,val in enumerate(word_list) if val=='am']\n",
    "        time_nn = []\n",
    "        #print len(amidxs)\n",
    "        for amidx in amidxs:\n",
    "            time_slot = []\n",
    "            idx_start = max(amidx-2, 0)\n",
    "            for ix in range(idx_start, amidx):\n",
    "                if word_list[ix].isdigit():\n",
    "                    time_slot.append(word_list[ix])\n",
    "            if len(time_slot) == 2:\n",
    "                time_value = int(time_slot[0])+int(time_slot[1])/60.0\n",
    "                time_nn.append(time_value)\n",
    "            if len(time_slot) == 1:\n",
    "                time_nn.append(int(time_slot[0]))\n",
    "        if time_nn:\n",
    "            return min(time_nn)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def extract_pm(word_list):\n",
    "    if 'pm' in word_list:\n",
    "        amidxs = [i for i,val in enumerate(word_list) if val=='pm']\n",
    "        time_nn = []\n",
    "        #print len(amidxs)\n",
    "        for amidx in amidxs:\n",
    "            time_slot = []\n",
    "            idx_start = max(amidx-2, 0)\n",
    "            for ix in range(idx_start, amidx):\n",
    "                if word_list[ix].isdigit():\n",
    "                    time_slot.append(word_list[ix])\n",
    "            if len(time_slot) == 2:\n",
    "                time_value = int(time_slot[0])+int(time_slot[1])/60.0\n",
    "                time_nn.append(time_value)\n",
    "            if len(time_slot) == 1:\n",
    "                time_nn.append(int(time_slot[0]))\n",
    "        if time_nn:\n",
    "            print word_list\n",
    "            return max(time_nn)\n",
    "        else:\n",
    "            return None\n",
    "for sen in t2[1]:\n",
    "    print extract_pm(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Split Large Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
