{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "import nltk\n",
    "import heapq\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import itertools\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "print stemmer.stem('parts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(n_clusters_, labels, corpus, mapping_dict, test_corpus):\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        # print indice_cluster\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        f1 = open('cluster_%s/%dth_cluster.txt' % (question_class, indice_cluster), 'w+')\n",
    "        for idx in idx_list:\n",
    "            f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "    f1.close()\n",
    "\n",
    "\n",
    "def write_ne(nes, question_class, mapping_dict):\n",
    "    f1 = open('ne_%s.txt' % question_class, 'w+')\n",
    "    for idx, doc in enumerate(nes):\n",
    "        f1.write('%s %s\\n' % (mapping_dict[idx], ' '.join(doc)))\n",
    "    f1.close()    \n",
    "\n",
    "def wmd_compute(x1, x2):\n",
    "    return wv.wmdistance(x1, x2)\n",
    "\n",
    "def analyze_cluster(n_clusters_, labels, corpus, mapping_dict, test_corpus, filename):\n",
    "    num = 0\n",
    "    other_content = []\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        if len(idx_list) > 4:\n",
    "            f1 = open('%s_%s/%dth_cluster.txt' % (filename, question_class, indice_cluster), 'w+')\n",
    "            for idx in idx_list:\n",
    "                f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "            f1.close()\n",
    "        if len(idx_list) < 4:\n",
    "            num = num + len(idx_list)\n",
    "            for idx in idx_list:\n",
    "                other_content.append(corpus[mapping_dict[idx] - 1])\n",
    "    f1 = open('%s_%s/other_cluster.txt' % (filename, question_class), 'w+')\n",
    "    for doc in other_content:\n",
    "        f1.write('%s\\n' % doc)\n",
    "    f1.close()\n",
    "    print num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of comments 1833\n",
      "length of new content 3986\n",
      "signle sentence with nn 3444\n",
      "single sentence without nn 496\n",
      "no comments 46\n"
     ]
    }
   ],
   "source": [
    "def rule_q9(sen, ne):\n",
    "    clean_ne = list(set(ne))\n",
    "    remove_words = ['car', 'vehicl','improv','dealership','custom','receiv','satisfact','respond','servic','time',\n",
    "                    'center','facil','feel','ok','tell','problem','pay','dealer','attent','hurri','condit','ant',\n",
    "                    'fine','deliver','get','question','deliveri','need','quality','day','amount','kind','chang',\n",
    "                    'honda','visit','told','speak','ask','requir','toyota','henc','place','area','filter','align',\n",
    "                    'compani','process','qualiti','care','outsid','complaint','manag','glass','inform','break','pad',\n",
    "                    'wash','clean','water','showroom','staff','month','year','side','break','oil','market','batteri',\n",
    "                    'pack','packag','product']\n",
    "    clean_ne = [word for word in clean_ne if word not in remove_words and len(word)>1]    \n",
    "    save_words = ['spare','reduc','reason','discount','extra','rupe','compar','differ','pay','payment','ac','part',\n",
    "                  'check','free','increas','high','low','less','more','costli','decreas','insur','explain']\n",
    "    clean_ne = list(set(clean_ne + [stemmer.stem(word) for word in sen.split() if stemmer.stem(word) in save_words]))\n",
    "    \n",
    "    # rules to merge keywords:\n",
    "    if 'rupe' in clean_ne:\n",
    "        clean_ne[clean_ne.index('rupe')] = 'charg'\n",
    "    if 'price' in clean_ne:\n",
    "        clean_ne[clean_ne.index('price')] = 'charg'        \n",
    "    if 'differ' in clean_ne:\n",
    "        clean_ne[clean_ne.index('differ')] = 'compar'        \n",
    "    if 'rate' in clean_ne:\n",
    "        clean_ne[clean_ne.index('rate')] = 'charg'\n",
    "    if 'pay' in clean_ne:\n",
    "        clean_ne[clean_ne.index('pay')] = 'charg' \n",
    "    if 'payment' in clean_ne:\n",
    "        clean_ne[clean_ne.index('payment')] = 'charg'         \n",
    "    if 'cost' in clean_ne:\n",
    "        clean_ne[clean_ne.index('cost')] = 'charg'\n",
    "    if 'amount' in clean_ne:\n",
    "        clean_ne[clean_ne.index('amount')] = 'charg'\n",
    "    if 'bill' in clean_ne:\n",
    "        clean_ne[clean_ne.index('bill')] = 'charg'           \n",
    "    if 'money' in clean_ne:\n",
    "        clean_ne[clean_ne.index('money')] = 'charg'\n",
    "    if 'low' in clean_ne:\n",
    "        clean_ne[clean_ne.index('low')] = 'reduc'\n",
    "    if 'decreas' in clean_ne:\n",
    "        clean_ne[clean_ne.index('decreas')] = 'reduc' \n",
    "    if 'less' in clean_ne:\n",
    "        clean_ne[clean_ne.index('less')] = 'reduc'  \n",
    "    if 'taxation' in clean_ne:\n",
    "        clean_ne[clean_ne.index('taxation')] = 'tax'\n",
    "    if 'more' in clean_ne:\n",
    "        clean_ne[clean_ne.index('more')] = 'costli'\n",
    "    if 'increas' in clean_ne:\n",
    "        clean_ne[clean_ne.index('increas')] = 'costli'\n",
    "    if 'increa' in clean_ne:\n",
    "        clean_ne[clean_ne.index('increa')] = 'costli'        \n",
    "    if 'high' in clean_ne:\n",
    "        clean_ne[clean_ne.index('high')] = 'costli'\n",
    "    if 'explain' in clean_ne:\n",
    "        clean_ne[clean_ne.index('explain')] = 'explan'   \n",
    "    if 'bumper' in clean_ne:\n",
    "        clean_ne[clean_ne.index('bumper')] = 'spare'\n",
    "    if 'part' in clean_ne:\n",
    "        clean_ne[clean_ne.index('part')] = 'spare'        \n",
    "    if 'tire' in clean_ne:\n",
    "        clean_ne[clean_ne.index('tire')] = 'spare'        \n",
    "    if 'ac' in clean_ne:\n",
    "        clean_ne[clean_ne.index('ac')] = 'spare'        \n",
    "    if 'vat' in clean_ne:\n",
    "        clean_ne[clean_ne.index('vat')] = 'tax'\n",
    "    if 'offer' in clean_ne:\n",
    "        clean_ne[clean_ne.index('offer')] = 'discount'        \n",
    "    clean_ne = list(set(clean_ne))\n",
    "\n",
    "    if len(clean_ne)>1 and 'costli' in clean_ne:\n",
    "        clean_ne.remove('costli')\n",
    "    if len(clean_ne)>1 and 'reduc' in clean_ne:\n",
    "        clean_ne.remove('reduc')\n",
    "    if len(clean_ne)>1 and 'charg' in clean_ne:\n",
    "        clean_ne.remove('charg')\n",
    "    if len(clean_ne)==1 and 'costli' in clean_ne:\n",
    "        clean_ne = ['charg']\n",
    "    if len(clean_ne)==1 and 'reduc' in clean_ne:\n",
    "        clean_ne = ['charg']\n",
    "    if 'tax' in clean_ne:\n",
    "        clean_ne= ['tax']\n",
    "    if 'labor' in clean_ne:\n",
    "        clean_ne= ['labor']        \n",
    "    if 'spare' in clean_ne:\n",
    "        clean_ne= ['spare']        \n",
    "    clean_ne = list(set(clean_ne))\n",
    "    return clean_ne\n",
    "\n",
    "def clean_corpus(x1):\n",
    "    x_new = []\n",
    "    for x in x1:\n",
    "        if 'no improvement' in x:\n",
    "            continue\n",
    "        else:\n",
    "            x_new.append(x)\n",
    "    return x_new\n",
    "\n",
    "\n",
    "def process_corpus(x1, pos_tags, general_stop,sent_ind): #new process_corpus func.===============================\n",
    "    # split three categories: 1 no improvemnt 2 with noun 3 others\n",
    "    doc_noimprove = []\n",
    "    doc_nn = []\n",
    "    nn_extracted = []\n",
    "    doc_other = []\n",
    "    r_ind = []\n",
    "    for ind,x in enumerate(x1):\n",
    "        if 'no improvement' in x:\n",
    "            doc_noimprove.append(x)\n",
    "            r_ind.append(ind)\n",
    "        else:\n",
    "            nn_list = []\n",
    "            sen = x\n",
    "            pos_new = nltk.pos_tag(nltk.word_tokenize(sen))\n",
    "            for token in pos_new:\n",
    "                if token[1] in pos_tags and not token[0] in general_stop:\n",
    "                    nn_list.append(token[0])\n",
    "            nn_list = [stemmer.stem(word) for word in nn_list] #stemming\n",
    "            nn_list = rule_q9(sen, nn_list) # apply rule\n",
    "                    \n",
    "            if nn_list != []:\n",
    "                nn_extracted.append(nn_list)\n",
    "                doc_nn.append(sen)\n",
    "            else:\n",
    "                doc_other.append(sen)\n",
    "                r_ind.append(ind)\n",
    "    sent_ind = [i for j, i in enumerate(sent_ind) if j not in r_ind]\n",
    "    return doc_noimprove, [doc_nn, nn_extracted], doc_other, sent_ind\n",
    "\n",
    "question_class = 'q9'\n",
    "fname = '..//dataset//%s.txt' % question_class\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "print 'length of comments', len(content)\n",
    "f.close()\n",
    "# split comment with multi-sentence into multi-comments\n",
    "content_new = []\n",
    "sent_to_comm_id = []\n",
    "for comment_ind, comment in enumerate(content):\n",
    "    sents = sent_tokenize(comment)\n",
    "    if len(sents) > 1:\n",
    "        for i in range(len(sents)):\n",
    "            content_new.append(sents[i])\n",
    "    else:\n",
    "        content_new.append(comment)\n",
    "    sent_to_comm_id += [comment_ind]*len(sents)\n",
    "\n",
    "\n",
    "nn_corpus = []\n",
    "# English stop words lists\n",
    "# stop_words = stopwords.words('english')\n",
    "stop_words = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself',\n",
    "            'yourselves','he','him','his','himself','she','her','good','fr','rs','hers','herself',\n",
    "             'it','its','itself','they','them','their','theirs','themselves','what','which','who',\n",
    "            'whom','this','that','these','those','am','is','time','something','are','was','were',\n",
    "            'be','been','do','does','did','doing','a','an','the','and','but','if','or','because',\n",
    "            'as','until','while','of','at','by','for','take','better','ve','with','about','against',\n",
    "            'into','through','during','before','to','from','in','out','on','off','over','under',\n",
    "            'again','further','then','once', 'after','didn','don','ft','have','had','has','having','had',\n",
    "            'here','there','when','where','why','how','all','any','both','each','few','most','other',\n",
    "            'some','such','only','doesn','sq','own','same','so','than','too','very','can','will','just',\n",
    "            'should','now','bit','anything','till','thing','things','toyota','hrs','km','sta','pm',\n",
    "            'everything','feedback','part','parts','issue','issues','ask','way','use','give','giving',\n",
    "            'gives','sometimes','focus','lot','work','works','need','needs','think','bring','people',\n",
    "            'person']\n",
    "punctuation_list = [unicode(i) for i in string.punctuation]\n",
    "\n",
    "for punctuation in punctuation_list:\n",
    "    stop_words.append(punctuation)\n",
    "\n",
    "pos_tags = ['NN', 'NNS']\n",
    "doc1, doc2, doc3, sent_comm_ind = process_corpus(content_new, pos_tags, stop_words, sent_to_comm_id)\n",
    "\n",
    "doc_nn, nn_extracted = doc2[0], doc2[1]\n",
    "print 'length of new content', len(content_new)\n",
    "print 'signle sentence with nn', len(doc_nn)\n",
    "print 'single sentence without nn', len(doc3)\n",
    "print 'no comments', len(doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({u'charg': 1311, 'spare': 435, 'labor': 342, u'compar': 164, 'tax': 143, 'discount': 124, 'free': 70, u'reason': 65, 'extra': 53, u'check': 39, u'insur': 28, u'materi': 10, 'card': 9, 'cash': 9, u'explan': 9, u'estim': 9, u'warranti': 9, 'call': 8, u'paint': 8, 'thousand': 8, u'expens': 8, u'scheme': 7, u'wheel': 6, 'etc': 6, u'citi': 5, u'system': 5, u'locat': 5, u'mainten': 5, u'knowledg': 5, 'board': 5, u'benefit': 5, u'damag': 5, 'brand': 5, 'harsha': 4, u'kilomet': 4, 'smile': 4, 'innova': 4, 'front': 4, 'drop': 4, u'noth': 4, 'period': 4, 'air': 4, 'charger': 4, u'percentag': 4, u'lakh': 4, u'run': 4, 'road': 4, 'detail': 4, u'chequ': 3, u'item': 3, 'pick': 3, 'door': 3, u'purpos': 3, 'share': 3, 'sunday': 3, 'lac': 3, 'counter': 3, 'maintain': 3, u'window': 3, 'clutch': 3, u'advanc': 3, 'kit': 3, 'date': 3, u'balanc': 3, u'engin': 3, u'class': 3, 'nut': 3, 'seat': 3, u'respons': 3, 'maruti': 3, 'point': 3, u'centr': 3, 'dust': 3, 'rest': 3, u'instrument': 3, u'transpar': 3, u'accessori': 3, 'mirror': 3, 'checkup': 3, 'repair': 3, u'lot': 3, 'percent': 3, u'appoint': 3, u'scratch': 2, u'sorri': 2, u'mechan': 2, u'level': 2, u'fortun': 2, u'respon': 2, 'opinion': 2, 'type': 2, u'peopl': 2, 'subburaj': 2, 'room': 2, 'mr': 2, 'minimum': 2, u'maintenc': 2, u'compulsori': 2, 'tier': 2, 'liner': 2, 'talk': 2, u'polici': 2, 'half': 2, 'name': 2, u'radiat': 2, 'receipt': 2, u'cleanli': 2, 'dsk': 2, 'correct': 2, u'situat': 2, u'basi': 2, u'rang': 2, 'number': 2, 'one': 2, 'nova': 2, u'conveni': 2, u'friend': 2, 'option': 2, 'shampoo': 2, 'chart': 2, 'plan': 2, 'sale': 2, u'km': 2, u'solut': 2, u'etio': 2, 'job': 2, 'height': 2, u'workshop': 2, 'interest': 2, u'worker': 2, u'bulb': 2, 'rs3000': 2, u'purchas': 2, u'instal': 2, 'hand': 2, u'consat': 2, u'exampl': 2, u'model': 2, 'speed': 2, u'other': 2, u'loss': 2, u'motor': 2, 'home': 2, u'inspect': 2, u'pictur': 2, 'ford': 2, 'line': 2, 'display': 2, 'ohm': 2, u'paddl': 2, 'sider': 2, 'rule': 2, u'merced': 1, 'alc': 1, 'skim': 1, 'settlement': 1, u'compact': 1, 'depend': 1, u'swat': 1, u'whichev': 1, 'exact': 1, 'list': 1, 'eve': 1, u'chrage': 1, u'verif': 1, 'tea': 1, 'lanson': 1, u'lightn': 1, u'chagr': 1, u'injustic': 1, 'hasn': 1, u'bodi': 1, 'hast': 1, u'becom': 1, u'valu': 1, 'balamurugan': 1, 'prior': 1, 'survey': 1, u'behav': 1, u'employe': 1, u'attitud': 1, u'garag': 1, u'famili': 1, 'marker': 1, 'aligarh': 1, 'total': 1, u'mobil': 1, 'eye': 1, u'carg': 1, 'doubt': 1, 'liva': 1, 'phone': 1, u'alin': 1, u'blade': 1, 'roof': 1, u'histori': 1, 'remain': 1, 'control': 1, 'claim': 1, u'onlin': 1, 'end': 1, u'secur': 1, 'delay': 1, u'simpl': 1, u'mat': 1, u'demonet': 1, u'inquiri': 1, 'law': 1, 'man': 1, u'villag': 1, 'sr': 1, 'around15000': 1, u'allot': 1, u'sm': 1, u'parti': 1, 'approx': 1, u'movi': 1, u'fit': 1, 'lamp': 1, 'jalpaiguri': 1, 'mail': 1, 'gadget': 1, u'automobil': 1, u'mention': 1, u'somebodi': 1, 'maruthi': 1, u'yr': 1, u'discuss': 1, u'term': 1, u'bonu': 1, u'separ': 1, u'mean': 1, 'schemer': 1, 'idea': 1, u'expect': 1, 'network': 1, 'profit': 1, 'dsr': 1, 're': 1, 'hyundai': 1, u'someon': 1, u'advisor': 1, u'afterward': 1, 'mahindra': 1, u'suspens': 1, 'infinium': 1, 'owner': 1, u'prioriti': 1, u'messag': 1, 'statement': 1, 'compressor': 1, 'tesal': 1, u'nobodi': 1, u'serv': 1, u'provid': 1, u'remov': 1, 'india': 1, u'toward': 1, u'provis': 1, 'pre': 1, 'clip': 1, 'defect': 1, u'sell': 1, u'screw': 1, u'advic': 1, 'interior': 1, 'gandhidham': 1, 'vise': 1, 'cover': 1, u'quantiti': 1, 'tech': 1, 'shop': 1, 'tkm': 1, u'shoe': 1, 'spite': 1, u'figur': 1, u'manufactur': 1, u'busi': 1, u'purcha': 1, 'sonak': 1, u'synthet': 1, 'twice': 1, 'husband': 1, 'attest': 1, 'label': 1, 'pickup': 1, 'paisa': 1, 'viper': 1, u'tyre': 1, u'concentr': 1, 'ghale': 1, 'larboard': 1, 'instant': 1, 'fault': 1, 'contract': 1, 'whole': 1, u'comment': 1, u'mistak': 1, 'trust': 1, 'belt': 1, u'due': 1, 'diesel': 1, 'fire': 1, 'amana': 1, u'coordin': 1, 'understand': 1, 'case': 1, 'look': 1, u'advantag': 1, 'cant': 1, 'brake': 1, u'develop': 1, 'trial': 1, u'perform': 1, u'suggest': 1, 'headlight': 1, 'member': 1, u'document': 1, u'minit': 1, 'week': 1, u'driver': 1, 'yesterday': 1, 'user': 1, u'refin': 1, 'thought': 1, u'distanc': 1, 'tobe': 1, u'languag': 1, 'wage': 1, 'versa': 1, 'govt': 1, 'world': 1, u'formal': 1, 'rs7500': 1, u'resid': 1, 'annamalai': 1, u'manpow': 1, 'inter': 1, u'deduct': 1, 'aliment': 1, 'proper': 1, u'nood': 1, 'scale': 1, 'per': 1, 'step': 1, 'bolt': 1, 'column': 1, 'book': 1, 'polimar': 1, 'brush': 1, 'fast': 1, 'hen': 1, 'gair': 1, u'concess': 1, 'link': 1, 'safeguard': 1, u'default': 1, u'troubl': 1, u'limit': 1, 'rs200': 1, u'cutom': 1, u'kilometr': 1, 'rubber': 1, u'valid': 1, 'lookup': 1, u'branch': 1, u'varieti': 1, 'depth': 1})\n",
      "majot list:\n",
      "[u'compar', 'extra', 'labor', 'free', u'reason', u'insur', u'charg', 'spare', 'tax', 'discount', u'check']\n",
      "comment ratio 88.9798145117\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def df_count(x1):\n",
    "    # split three categories: 1 no improvemnt 2 with noun 3 others\n",
    "    text = []\n",
    "    for ab in x1:\n",
    "        text = text + ab\n",
    "    df = Counter(text)\n",
    "    return df\n",
    "\n",
    "def filter_ne(test_corpus, df):  # assuming each review contain one aspect\n",
    "    for xth, doc in enumerate(test_corpus):\n",
    "        if len(doc)>1:\n",
    "            df_words = [df[word] for word in doc]\n",
    "            idx =  heapq.nlargest(1, xrange(len(df_words)), key=df_words.__getitem__)\n",
    "            test_corpus[xth] = [stemmer.stem(doc[ith]) for ith in idx]\n",
    "    return test_corpus\n",
    "\n",
    "def write_file(corpus, idx_list, word):\n",
    "    f1 = open('cluster/%s/%s_comment.txt' % (word, word), 'w+')\n",
    "    for idx in idx_list:\n",
    "        f1.write('%s\\n' %corpus[idx])\n",
    "    f1.close()\n",
    "    \n",
    "    \n",
    "def main_category(df_list, nn_clean, corpus,sc_ind): # new main_cate func. with common index\n",
    "    if (not os.path.isdir(\"cluster\")):\n",
    "        os.mkdir(\"cluster\")\n",
    "    name_list = {}\n",
    "    major_list = [word for word in df_list if df_list[word]>10]\n",
    "    print \"majot list:\\n\", major_list\n",
    "    scidx_set = []\n",
    "    for word in major_list:\n",
    "        if not os.path.isdir(\"cluster/%s\" %word):\n",
    "            os.mkdir(\"cluster/%s\" %word)\n",
    "        idx_set = []\n",
    "        for idx, doc in enumerate(nn_clean):\n",
    "            if word in doc:\n",
    "                idx_set.append(idx)\n",
    "        write_file(doc_nn, idx_set, word)\n",
    "        name_list[word] = idx_set\n",
    "        scidx_set += [sc_ind[i] for i in idx_set]\n",
    "    return name_list, len(set(scidx_set))\n",
    "\n",
    "df = df_count(nn_extracted)\n",
    "nn_clean = filter_ne(nn_extracted, df)\n",
    "df = df_count(nn_clean)\n",
    "print df\n",
    "dict_map = dict(df.most_common())\n",
    "name_list,nb_comm = main_category(dict_map, nn_clean,doc_nn,sent_comm_ind)\n",
    "\n",
    "print 'comment ratio', float(nb_comm)/len(content)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Large Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['reduced'], [], [], ['periodic'], ['many', 'higher'], [], ['just'], [], ['high'], ['stuck', 'center', 'all'], ['all'], ['decreased'], ['high'], [], ['high', 'regular'], [], ['high', 'low'], [], ['other', 'like'], ['high', 'same']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import product\n",
    "def sim_check_word(word1,word2):\n",
    "    syns1 = wn.synsets(word1)\n",
    "    syns2 = wn.synsets(word2)\n",
    "    sims = []\n",
    "    for sense1, sense2 in product(syns1, syns2):\n",
    "        d = wn.path_similarity(sense1, sense2)\n",
    "        sims.append((d))\n",
    "    return max(sims)\n",
    "\n",
    "def sim_check_list(list1,list2):\n",
    "    sims = []\n",
    "    for word in list1:\n",
    "        for word2 in list2:\n",
    "            sims.append(sim_check_word(word, word2))\n",
    "    return max(sims)\n",
    "\n",
    "\n",
    "def l2_extract(corpus, idx_list):\n",
    "    num = 0\n",
    "    other_content = []\n",
    "    adj_batchlist = []\n",
    "    local_content = []\n",
    "    for idx in idx_list:\n",
    "        doc = corpus[idx]\n",
    "        local_content.append(doc)\n",
    "        adj_list = []\n",
    "        for word in doc.split():\n",
    "            try:\n",
    "                tmp = [wn.synsets(word)[hh].pos() for hh in range(len(wn.synsets(word)))] \n",
    "            except IndexError:\n",
    "                tmp = None\n",
    "            if 'a' in tmp:\n",
    "                adj_list.append(word)\n",
    "        adj_batchlist.append(adj_list)  \n",
    "    return adj_batchlist, local_content\n",
    "\n",
    "def space_split_run(adj_list, set_rule):\n",
    "    labels = []\n",
    "    for tmp_set in adj_list:\n",
    "        if len(tmp_set) > 0:\n",
    "            scores = []\n",
    "            for rule_list in set_rule:\n",
    "                scores.append(sim_check_list(tmp_set, rule_list))\n",
    "            if max(scores) > 0.7:\n",
    "                labels.append(scores.index(max(scores)))\n",
    "            else:\n",
    "                labels.append(1)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    return labels\n",
    "\n",
    "def write_all(content_list, idx_labels, keyword):\n",
    "    for i in range(-1, max(idx_labels)+2):\n",
    "        f1 = open('cluster/%s/%s_%d.txt' %(keyword, keyword,i),  'w+')\n",
    "        for idx, doc in enumerate(content_list):\n",
    "            if idx_labels[idx] == i:\n",
    "                f1.write('%s\\n' % (content_list[idx]))\n",
    "        f1.close()\n",
    "\n",
    "tt_list, local_content = l2_extract(doc_nn, name_list['charg'])\n",
    "print tt_list[:20]\n",
    "# set_rule = [['labor', 'expens', 'reduc'], ['tax', 'high', 'reduc'], ['wash', 'extra']]\n",
    "# idx_labels = space_split_run(tt_list,set_rule)\n",
    "# write_all(local_content, idx_labels, 'charg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
