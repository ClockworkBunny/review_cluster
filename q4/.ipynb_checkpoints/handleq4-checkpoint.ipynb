{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "import nltk\n",
    "import heapq\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import itertools\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "print stemmer.stem('clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(n_clusters_, labels, corpus, mapping_dict, test_corpus):\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        # print indice_cluster\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        f1 = open('cluster_%s/%dth_cluster.txt' % (question_class, indice_cluster), 'w+')\n",
    "        for idx in idx_list:\n",
    "            f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "    f1.close()\n",
    "\n",
    "\n",
    "def write_ne(nes, question_class, mapping_dict):\n",
    "    f1 = open('ne_%s.txt' % question_class, 'w+')\n",
    "    for idx, doc in enumerate(nes):\n",
    "        f1.write('%s %s\\n' % (mapping_dict[idx], ' '.join(doc)))\n",
    "    f1.close()    \n",
    "\n",
    "def wmd_compute(x1, x2):\n",
    "    return wv.wmdistance(x1, x2)\n",
    "\n",
    "def analyze_cluster(n_clusters_, labels, corpus, mapping_dict, test_corpus, filename):\n",
    "    num = 0\n",
    "    other_content = []\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        if len(idx_list) > 4:\n",
    "            f1 = open('%s_%s/%dth_cluster.txt' % (filename, question_class, indice_cluster), 'w+')\n",
    "            for idx in idx_list:\n",
    "                f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "            f1.close()\n",
    "        if len(idx_list) < 4:\n",
    "            num = num + len(idx_list)\n",
    "            for idx in idx_list:\n",
    "                other_content.append(corpus[mapping_dict[idx] - 1])\n",
    "    f1 = open('%s_%s/other_cluster.txt' % (filename, question_class), 'w+')\n",
    "    for doc in other_content:\n",
    "        f1.write('%s\\n' % doc)\n",
    "    f1.close()\n",
    "    print num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of comments 1146\n",
      "length of new content 2725\n",
      "signle sentence with nn 2205\n",
      "single sentence without nn 500\n",
      "no comments 20\n"
     ]
    }
   ],
   "source": [
    "def rule_q4(sen, ne):\n",
    "    clean_ne = list(set(ne))\n",
    "    remove_words = ['car', 'vehicl','improv','dealership','custom','receiv','satisfact','respond','servic','time',\n",
    "                    'innova','center','facil','feel','ok','tell','problem','pay','dealer','attent','hurri','condit',\n",
    "                    'ant','fine','deliver','get','question','deliveri','need','quality','day','side','kind','chang',\n",
    "                    'honda','visit','told','speak','ask','requir','maruti','cleanli','henc','place','area','hand',\n",
    "                    'compani','process','qualiti','care','outsid','complaint','depart','hour','wait','front','home',\n",
    "                    'centr','system']\n",
    "    clean_ne = [word for word in clean_ne if word not in remove_words and len(word)>1]    \n",
    "    save_words = ['polish','wash','interior','extra','rupe','check','vacuum','clean','downsid','intern'\n",
    "                  'insid','ac','dry','engin','inter']\n",
    "    uni_words = ['mat','interior','charg']\n",
    "    clean_ne = list(set(clean_ne + [stemmer.stem(word) for word in sen.split() if stemmer.stem(word) in save_words]))\n",
    "    \n",
    "    # rules to merge keywords:\n",
    "    # rules to seperate keywords:\n",
    "    if 'rupe' in clean_ne:\n",
    "        clean_ne[clean_ne.index('rupe')] = 'charg'\n",
    "    if 'rate' in clean_ne:\n",
    "        clean_ne[clean_ne.index('rate')] = 'charg'        \n",
    "    if 'cost' in clean_ne:\n",
    "        clean_ne[clean_ne.index('cost')] = 'charg'\n",
    "    if 'money' in clean_ne:\n",
    "        clean_ne[clean_ne.index('money')] = 'charg'\n",
    "    if 'insid' in clean_ne:\n",
    "        clean_ne[clean_ne.index('insid')] = 'interior'\n",
    "    if 'intern' in clean_ne:\n",
    "        clean_ne[clean_ne.index('insid')] = 'interior'\n",
    "    if 'window' in clean_ne:\n",
    "        clean_ne[clean_ne.index('window')] = 'glass'\n",
    "    if 'dirt' in clean_ne:\n",
    "        clean_ne[clean_ne.index('dirt')] = 'dust'  \n",
    "    if 'manag' in clean_ne:\n",
    "        clean_ne[clean_ne.index('manag')] = 'staff'\n",
    "    if 'advisor' in clean_ne:\n",
    "        clean_ne[clean_ne.index('advisor')] = 'staff'  \n",
    "    if 'supervisor' in clean_ne:\n",
    "        clean_ne[clean_ne.index('supervisor')] = 'staff'  \n",
    "    if 'labor' in clean_ne:\n",
    "        clean_ne[clean_ne.index('labor')] = 'worker'      \n",
    "    if 'cloth' in clean_ne:\n",
    "        clean_ne[clean_ne.index('cloth')] = 'dri'\n",
    "    if 'vacuum' in clean_ne:\n",
    "        clean_ne[clean_ne.index('vacuum')] = 'dri'        \n",
    "    if 'water' in clean_ne:\n",
    "        clean_ne[clean_ne.index('water')] = 'wash'\n",
    "    if 'spot' in clean_ne:\n",
    "        clean_ne[clean_ne.index('spot')] = 'stain'          \n",
    "    if 'water' in clean_ne and 'stain' in clean_ne:\n",
    "        clean_ne[clean_ne.index('water')] = 'stain' \n",
    "    if len(clean_ne) > 1 and 'clean' in clean_ne:\n",
    "        clean_ne.remove('clean')  \n",
    "        \n",
    "    uniwrd = [cn for cn in clean_ne if cn in uni_words]\n",
    "    if uniwrd != []:\n",
    "        clean_ne = uniwrd\n",
    "    clean_ne = list(set(clean_ne))\n",
    "    return clean_ne\n",
    "\n",
    "def clean_corpus(x1):\n",
    "    x_new = []\n",
    "    for x in x1:\n",
    "        if 'no improvement' in x:\n",
    "            continue\n",
    "        else:\n",
    "            x_new.append(x)\n",
    "    return x_new\n",
    "\n",
    "\n",
    "def process_corpus(x1, pos_tags, general_stop):\n",
    "    # split three categories: 1 no improvemnt 2 with noun 3 others\n",
    "    doc_noimprove = []\n",
    "    doc_nn = []\n",
    "    nn_extracted = []\n",
    "    doc_other = []\n",
    "    for x in x1:\n",
    "        if 'no improvement' in x:\n",
    "            doc_noimprove.append(x)\n",
    "        else:\n",
    "            nn_list = []\n",
    "            sen = x\n",
    "            pos_new = nltk.pos_tag(nltk.word_tokenize(sen))\n",
    "            for token in pos_new:\n",
    "                if token[1] in pos_tags and not token[0] in general_stop:\n",
    "                    nn_list.append(token[0])\n",
    "            nn_list = [stemmer.stem(word) for word in nn_list] #stemming\n",
    "            nn_list = rule_q4(sen, nn_list) # apply rule\n",
    "                    \n",
    "            if nn_list != []:\n",
    "                nn_extracted.append(nn_list)\n",
    "                doc_nn.append(sen)\n",
    "            else:\n",
    "                doc_other.append(sen)\n",
    "    return doc_noimprove, [doc_nn, nn_extracted], doc_other\n",
    "        \n",
    "\n",
    "\n",
    "question_class = 'q4'\n",
    "fname = '..//dataset//%s.txt' % question_class\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "print 'length of comments', len(content)\n",
    "f.close()\n",
    "# split comment with multi-sentence into multi-comments\n",
    "content_new = []\n",
    "for comment in content:\n",
    "    sents = sent_tokenize(comment)\n",
    "    if len(sents) > 1:\n",
    "        for i in range(len(sents)):\n",
    "            content_new.append(sents[i])\n",
    "    else:\n",
    "        content_new.append(comment)\n",
    "print 'length of new content', len(content_new)\n",
    "# print content_new\n",
    "\n",
    "nn_corpus = []\n",
    "# English stop words lists\n",
    "# stop_words = stopwords.words('english')\n",
    "stopwords = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself',\n",
    "            'yourselves','he','him','his','himself','she','her','good','fr','rs','hers','herself',\n",
    "            'it','its','itself','they','them','their','theirs','themselves','what','which','who',\n",
    "            'whom','this','that','these','those','am','is','time','something','are','was','were',\n",
    "            'be','been','do','does','did','doing','a','an','the','and','but','if','or','because',\n",
    "            'as','until','while','of','at','by','for','take','better','ve','with','about','against',\n",
    "            'into','through','during','before','to','from','in','out','on','off','over','under',\n",
    "            'again','further','then','once', 'after','didn','don','ft','have','had','has','having','had',\n",
    "            'here','there','when','where','why','how','all','any','both','each','few','more','most','other',\n",
    "            'some','such','only','doesn','sq','own','same','so','than','too','very','can','will','just',\n",
    "            'should','now','bit','anything','till','thing','things','toyota','hrs','km','sta','pm',\n",
    "            'everything','feedback','part','parts','issue','issues','ask','way','use','give','giving',\n",
    "            'gives','sometimes','focus','lot','work','works','need','needs','think','bring','people',\n",
    "            'person']\n",
    "punctuation_list = [unicode(i) for i in string.punctuation]\n",
    "\n",
    "for punctuation in punctuation_list:\n",
    "    stop_words.append(punctuation)\n",
    "\n",
    "pos_tags = ['NN', 'NNS']\n",
    "doc1, doc2, doc3 = process_corpus(content_new, pos_tags, stop_words)\n",
    "\n",
    "doc_nn, nn_extracted = doc2[0], doc2[1]\n",
    "print 'signle sentence with nn', len(doc_nn)\n",
    "print 'single sentence without nn', len(doc3)\n",
    "print 'no comments', len(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({u'wash': 569, 'clean': 492, 'staff': 151, 'check': 108, 'interior': 105, 'dust': 69, u'charg': 46, u'polish': 42, 'dri': 33, u'glass': 30, u'stain': 28, 'worker': 23, 'seat': 15, u'engin': 11, 'door': 10, 'oil': 10, 'mat': 10, 'tire': 9, u'downsid': 7, 'call': 6, u'locat': 5, 'board': 5, 'mirror': 5, 'dashboard': 5, 'point': 5, u'toward': 5, 'look': 5, 'year': 5, u'drive': 5, 'product': 5, 'driver': 5, u'mechan': 4, 'proper': 4, 'mud': 4, u'chemic': 4, 'showroom': 4, 'inter': 4, 'road': 4, 'bonnet': 4, u'align': 4, 'ac': 4, u'respons': 4, 'man': 4, u'person': 4, 'client': 4, u'execut': 3, 'paper': 3, u'paint': 3, 'night': 3, 'space': 3, 'diesel': 3, u'bodi': 3, u'respon': 3, u'materi': 3, 'trial': 3, 'load': 3, 'mark': 3, u'morn': 3, u'dirti': 3, 'manner': 3, u'sit': 3, 'wheel': 3, 'payment': 3, 'portion': 3, u'explan': 2, 'month': 2, 'brain': 2, 'touch': 2, u'concentr': 2, 'extra': 2, 'return': 2, u'term': 2, u'experi': 2, 'twice': 2, 'follow': 2, u'manpow': 2, 'rate': 2, u'expect': 2, u'handov': 2, u'wire': 2, 'market': 2, 'carpet': 2, u'machin': 2, 'spare': 2, 'outer': 2, 'come': 2, u'purpo': 2, u'boy': 2, 'fault': 2, u'machan': 2, 'period': 2, u'batteri': 2, u'mistak': 2, 'tomorrow': 2, u'deepli': 2, u'type': 2, 'minor': 2, 'ram': 2, u'equip': 2, 'brand': 2, 'technician': 2, 'number': 2, u'present': 2, 'bill': 2, u'whatev': 2, 'inform': 2, u'detail': 2, u'resourc': 2, 'end': 2, 'infant': 1, u'distanc': 1, u'purchas': 1, 'dickey': 1, 'crista': 1, u'photo': 1, u'nowaday': 1, u'failur': 1, 'connect': 1, u'shoe': 1, u'scrack': 1, 'chair': 1, 'note': 1, u'polici': 1, 'graph': 1, u'cleaness': 1, u'offic': 1, u'hou': 1, u'anyon': 1, u'formal': 1, 'plate': 1, u'transpar': 1, 'delhi': 1, 'shilfata': 1, u'scratech': 1, 'dehradun': 1, 'break': 1, 'mention': 1, 'desk': 1, 'word': 1, 'room': 1, u'loss': 1, 'level': 1, 'clutch': 1, 'boarder': 1, 'list': 1, 'patch': 1, 'stuff': 1, u'min': 1, 'guy': 1, u'complet': 1, u'pleas': 1, 'week': 1, u'fortun': 1, 'tea': 1, u'knowledg': 1, u'procedur': 1, u'noth': 1, u'finish': 1, 'logo': 1, u'tier': 1, u'throughli': 1, u'even': 1, u'entranc': 1, u'compar': 1, 'section': 1, u'leav': 1, 'correct': 1, 'gate': 1, 'screen': 1, 'dikki': 1, 'hyundai': 1, u'compulsori': 1, 'discount': 1, 'job': 1, 'base': 1, 'key': 1, 'madhuban': 1, u'fit': 1, u'grous': 1, 'reason': 1, u'cleanl': 1, 'bumper': 1, u'vechicl': 1, 'share': 1, 'repair': 1, 'dick': 1, 'color': 1, u'import': 1, 'pot': 1, u'clariti': 1, 'one': 1, u'instruct': 1, 'weather': 1, 'brush': 1, 'infinium': 1, u'open': 1, u'carless': 1, u'vichl': 1, 'sheet': 1, u'accumul': 1, 'sake': 1, u'cleaniless': 1, 'tata': 1, u'capac': 1, u'citi': 1, u'silenc': 1, 'workshop': 1, u'lot': 1, u'accessori': 1, 'life': 1, 'gear': 1, u'villag': 1, u'peopl': 1, u'copi': 1, 'ford': 1, 'behalf': 1, u'reentri': 1, u'greas': 1, 'batter': 1, u'accid': 1, 'wise': 1, 'placer': 1, u'wiper': 1, 'roof': 1, 'air': 1, 'record': 1, u'remain': 1, 'behavior': 1, u'posit': 1, u'arrang': 1, u'minut': 1, u'everyon': 1, u'steer': 1, 'lanson': 1, 'streak': 1, u'featur': 1, 'ad': 1, 'soil': 1, 'mind': 1, 'deep': 1, u'cant': 1, u'motiv': 1, u'underbodi': 1, u'advi': 1, 'ship': 1, u'tidi': 1, 'bhilar': 1, u'techniqu': 1, u'suggest': 1, 'travel': 1, u'headlight': 1, u'member': 1, 'labour': 1, u'roadsid': 1, u'advis': 1, u'ignor': 1, u'thousand': 1, 'donot': 1, 'dambar': 1, u'statu': 1, u'express': 1, u'action': 1, u'rais': 1, u'supervi': 1, u'greec': 1, 'vaccum': 1, u'appoint': 1, u'dark': 1, u'excus': 1, u'weld': 1, 'liquid': 1, 'charger': 1, 'bottom': 1, 'request': 1, 'sale': 1, u'rule': 1, 'face': 1, 'pipe': 1, u'green': 1, u'dot': 1, 'traffic': 1, 're': 1})\n",
      "majot list:\n",
      "[u'polish', 'staff', 'tire', u'stain', 'seat', 'dri', u'wash', u'downsid', 'call', 'door', 'worker', u'glass', 'check', u'engin', 'interior', 'oil', 'mat', 'dust', u'charg', 'clean']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def df_count(x1):\n",
    "    # split three categories: 1 no improvemnt 2 with noun 3 others\n",
    "    text = []\n",
    "    for ab in x1:\n",
    "        text = text + ab\n",
    "    df = Counter(text)\n",
    "    return df\n",
    "\n",
    "def filter_ne(test_corpus, df):  # assuming each review contain one aspect\n",
    "    for xth, doc in enumerate(test_corpus):\n",
    "        if len(doc)>1:\n",
    "            df_words = [df[word] for word in doc]\n",
    "            idx =  heapq.nlargest(1, xrange(len(df_words)), key=df_words.__getitem__)\n",
    "            test_corpus[xth] = [stemmer.stem(doc[ith]) for ith in idx]\n",
    "    return test_corpus\n",
    "\n",
    "def write_file(corpus, idx_list, word):\n",
    "    f1 = open('cluster/%s/%s_comment.txt' % (word, word), 'w+')\n",
    "    for idx in idx_list:\n",
    "        f1.write('%s\\n' %corpus[idx])\n",
    "    f1.close()\n",
    "    \n",
    "    \n",
    "def main_category(df_list, nn_clean, corpus):\n",
    "    if (not os.path.isdir(\"cluster\")):\n",
    "        os.mkdir(\"cluster\")\n",
    "    name_list = {}\n",
    "    major_list = [word for word in df_list if df_list[word]>5]\n",
    "    print \"majot list:\\n\", major_list\n",
    "    for word in major_list:\n",
    "        if not os.path.isdir(\"cluster/%s\" %word):\n",
    "            os.mkdir(\"cluster/%s\" %word)\n",
    "        idx_set = []\n",
    "        for idx, doc in enumerate(nn_clean):\n",
    "            if word in doc:\n",
    "                idx_set.append(idx)\n",
    "        write_file(doc_nn, idx_set, word)\n",
    "        name_list[word] = idx_set\n",
    "    return name_list\n",
    "\n",
    "df = df_count(nn_extracted)\n",
    "nn_clean = filter_ne(nn_extracted, df)\n",
    "df = df_count(nn_clean)\n",
    "print df\n",
    "dict_map = dict(df.most_common())\n",
    "name_list = main_category(dict_map, nn_clean,doc_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Large Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import product\n",
    "def sim_check_word(word1,word2):\n",
    "    syns1 = wn.synsets(word1)\n",
    "    syns2 = wn.synsets(word2)\n",
    "    sims = []\n",
    "    for sense1, sense2 in product(syns1, syns2):\n",
    "        d = wn.path_similarity(sense1, sense2)\n",
    "        sims.append((d))\n",
    "    return max(sims)\n",
    "\n",
    "def sim_check_list(list1,list2):\n",
    "    sims = []\n",
    "    for word in list1:\n",
    "        for word2 in list2:\n",
    "            sims.append(sim_check_word(word, word2))\n",
    "    return max(sims)\n",
    "\n",
    "\n",
    "def l2_extract(corpus, idx_list):\n",
    "    num = 0\n",
    "    other_content = []\n",
    "    adj_batchlist = []\n",
    "    local_content = []\n",
    "    for idx in idx_list:\n",
    "        doc = corpus[idx]\n",
    "        local_content.append(doc)\n",
    "        adj_list = []\n",
    "        for word in doc.split():\n",
    "            try:\n",
    "                tmp = [wn.synsets(word)[hh].pos() for hh in range(len(wn.synsets(word)))] \n",
    "            except IndexError:\n",
    "                tmp = None\n",
    "            if 'a' in tmp:\n",
    "                adj_list.append(word)\n",
    "        adj_batchlist.append(adj_list)\n",
    "    \n",
    "    \n",
    "    return adj_batchlist, local_content\n",
    "\n",
    "# tt_list, local_content = l2_extract(doc_nn, name_list['clean'])\n",
    "# print tt_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
