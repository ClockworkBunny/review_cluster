{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import itertools\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#filter(None, re.split('(-|:|am|pm)', '30'))\n",
    "\n",
    "\n",
    "#loading word embeddings\n",
    "#wv = KeyedVectors.load_word2vec_format(\"../w2v/w2vemb.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(n_clusters_, labels, corpus, mapping_dict, test_corpus):\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        # print indice_cluster\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        f1 = open('cluster_%s/%dth_cluster.txt' % (question_class, indice_cluster), 'w+')\n",
    "        for idx in idx_list:\n",
    "            f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "    f1.close()\n",
    "\n",
    "\n",
    "def write_ne(nes, question_class, mapping_dict):\n",
    "    f1 = open('ne_%s.txt' % question_class, 'w+')\n",
    "    for idx, doc in enumerate(nes):\n",
    "        f1.write('%s %s\\n' % (mapping_dict[idx], ' '.join(doc)))\n",
    "    f1.close()    \n",
    "\n",
    "def wmd_compute(x1, x2):\n",
    "    return wv.wmdistance(x1, x2)\n",
    "\n",
    "def analyze_cluster(n_clusters_, labels, corpus, mapping_dict, test_corpus, filename):\n",
    "    num = 0\n",
    "    other_content = []\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        if len(idx_list) > 4:\n",
    "            f1 = open('%s_%s/%dth_cluster.txt' % (filename, question_class, indice_cluster), 'w+')\n",
    "            for idx in idx_list:\n",
    "                f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "            f1.close()\n",
    "        if len(idx_list) < 4:\n",
    "            num = num + len(idx_list)\n",
    "            for idx in idx_list:\n",
    "                other_content.append(corpus[mapping_dict[idx] - 1])\n",
    "    f1 = open('%s_%s/other_cluster.txt' % (filename, question_class), 'w+')\n",
    "    for doc in other_content:\n",
    "        f1.write('%s\\n' % doc)\n",
    "    f1.close()\n",
    "    print num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of comments 478\n"
     ]
    }
   ],
   "source": [
    "question_class = 'q2'\n",
    "fname = '..//data//%s.txt' % question_class\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "print 'length of comments', len(content)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Reviews based on time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_corpus(x1):\n",
    "    x_new = []\n",
    "    for x in x1:\n",
    "        if 'no improvement' in x:\n",
    "            continue\n",
    "        else:\n",
    "            x_new.append(x)\n",
    "    return x_new\n",
    "\n",
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "def process_corpus(x1):\n",
    "    # split three categories: 1 no improvemnt 2 with noun 3 others\n",
    "    doc_noimprove = []\n",
    "    doc_single = []\n",
    "    for idx, x in enumerate(x1):\n",
    "        if 'no improvement' in x or 'respondent' in x:\n",
    "            doc_noimprove.append((x, idx))\n",
    "        else:\n",
    "            sents = sent_tokenize(x)\n",
    "            if len(sents) > 1:\n",
    "                for sen in sents:\n",
    "                    doc_single.append((sen, idx))\n",
    "            else:\n",
    "                doc_single.append((sents[0], idx))\n",
    "    return doc_noimprove, doc_single\n",
    "    #return doc_noimprove, [doc_nn, nn_extracted], doc_other, doc_multi\n",
    "\n",
    "def process_sentence(x1, day_senswords, time_senswords, general_stop):\n",
    "    # split three categories: 1 no improvemnt 2 with noun 3 others\n",
    "    doc_noimprove = []\n",
    "    doc_days = []\n",
    "    doc_time = []\n",
    "    doc_other = []\n",
    "    doc_nn = []\n",
    "    for x in x1:\n",
    "        nn_list = []\n",
    "        sen = x[0]\n",
    "        word_list = nltk.word_tokenize(sen)\n",
    "        if not set(word_list).isdisjoint(day_senswords):\n",
    "            doc_days.append((sen, x[1]))\n",
    "        else:\n",
    "            clean_word_list = []\n",
    "            for word in word_list:\n",
    "                clean_word_list = clean_word_list + filter(None, re.split('(-|:|am|pm)', word)) \n",
    "            if 'am' in clean_word_list or 'pm' in clean_word_list or num_there(clean_word_list):\n",
    "                doc_time.append((sen, x[1]))\n",
    "            else:\n",
    "                doc_other.append((sen, x[1]))\n",
    "\n",
    "    return doc_days, doc_time, doc_other\n",
    "nn_corpus = []\n",
    "\n",
    "t_noimporve, t_single = process_corpus(content)\n",
    "\n",
    "\n",
    "# English stop words lists\n",
    "stop_words = stopwords.words('english')\n",
    "punctuation_list = [unicode(i) for i in string.punctuation]\n",
    "\n",
    "for punctuation in punctuation_list:\n",
    "    stop_words.append(punctuation)\n",
    "\n",
    "day_sense = ['sunday', 'sundays', 'weekend', 'weekends', 'holidays', 'holiday']\n",
    "time_sense = ['am', 'pm',]\n",
    "# return day-covered, time-covered, and others data\n",
    "review_day, review_time, review_other = process_sentence(t_single, day_sense, time_sense, stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_am(word_list):\n",
    "    if 'am' in word_list:\n",
    "        amidxs = [i for i,val in enumerate(word_list) if val=='am']\n",
    "        time_nn = []\n",
    "        #print len(amidxs)\n",
    "        for amidx in amidxs:\n",
    "            time_slot = []\n",
    "            idx_start = max(amidx-2, 0)\n",
    "            for ix in range(idx_start, amidx):\n",
    "                if word_list[ix].isdigit():\n",
    "                    time_slot.append(word_list[ix])\n",
    "            if len(time_slot) == 2:\n",
    "                time_value = float(time_slot[0])+int(time_slot[1])/60.0\n",
    "                time_nn.append(time_value)\n",
    "            if len(time_slot) == 1:\n",
    "                time_nn.append(float(time_slot[0]))\n",
    "        if time_nn:\n",
    "            return min(time_nn)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def extract_pm(word_list):\n",
    "    if 'pm' in word_list:\n",
    "        amidxs = [i for i,val in enumerate(word_list) if val=='pm']\n",
    "        time_nn = []\n",
    "        #print len(amidxs)\n",
    "        for amidx in amidxs:\n",
    "            time_slot = []\n",
    "            idx_start = max(amidx-2, 0)\n",
    "            for ix in range(idx_start, amidx):\n",
    "                if word_list[ix].isdigit():\n",
    "                    time_slot.append(word_list[ix])\n",
    "            if len(time_slot) == 2:\n",
    "                time_value = int(time_slot[0])+int(time_slot[1])/60.0\n",
    "                time_nn.append(float(time_value))\n",
    "            if len(time_slot) == 1:\n",
    "                time_nn.append(float(time_slot[0]))\n",
    "        if time_nn:\n",
    "            #print word_list\n",
    "            return max(time_nn)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def time_extract(sen):\n",
    "    word_list = nltk.word_tokenize(sen[0])\n",
    "    clean_word_list = []\n",
    "    for word in word_list:\n",
    "        clean_word_list = clean_word_list + filter(None, re.split('(-|:|am|pm)', word)) \n",
    "    return (extract_am(clean_word_list), extract_pm(clean_word_list))\n",
    "\n",
    "def day_extract(sen): \n",
    "    word_list = nltk.word_tokenize(sen[0])\n",
    "    class_0 = ['sunday', 'sundays']\n",
    "    class_1 = ['weekend', 'weekends']\n",
    "    class_2 = ['holidays', 'holiday']\n",
    "    if not set(word_list).isdisjoint(class_0):\n",
    "        return class_0[0]\n",
    "    if not set(word_list).isdisjoint(class_1):\n",
    "        return class_1[0]\n",
    "    if not set(word_list).isdisjoint(class_2):\n",
    "        return class_2[0]\n",
    "\n",
    "\n",
    "for idx, sing_review in enumerate(review_day):\n",
    "    review_day[idx] = (sing_review[0], sing_review[1], day_extract(sing_review))\n",
    "\n",
    "for idx, sing_review in enumerate(review_time):\n",
    "    review_time[idx] = (sing_review[0], sing_review[1], time_extract(sing_review))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of comments 478\n",
      "# of sents 1060\n",
      "# of clusters 51\n",
      "sent ratio 0.625471698113\n",
      "comment ratio 0.830543933054\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_sen = len(review_day+review_time+t_noimporve)\n",
    "\n",
    "review_id = [sig[1] for sig in review_day] + [sig[1] for sig in review_time] + [sig[1] for sig in t_noimporve]\n",
    "review_cat =  [sig[2] for sig in review_day] + [sig[2] for sig in review_time]\n",
    "review_id = set(review_id)\n",
    "review_cat = set(review_cat)\n",
    "\n",
    "print '# of comments', len(content)\n",
    "print '# of sents', (num_sen+len(review_other))\n",
    "print '# of clusters', len(review_cat) + 1  # add no improve\n",
    "print 'sent ratio', float(num_sen)/(num_sen+len(review_other))\n",
    "print 'comment ratio', len(review_id)/float(len(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Split Large Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
