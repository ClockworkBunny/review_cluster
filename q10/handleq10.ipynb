{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import itertools\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import *\n",
    "import re\n",
    "#import pyemd\n",
    "#loading word embeddings\n",
    "#wv = KeyedVectors.load_word2vec_format(\"../w2v/w2vemb.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(n_clusters_, labels, corpus, mapping_dict, test_corpus):\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        # print indice_cluster\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        f1 = open('cluster_%s/%dth_cluster.txt' % (question_class, indice_cluster), 'w+')\n",
    "        for idx in idx_list:\n",
    "            f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "    f1.close()\n",
    "\n",
    "\n",
    "def write_ne(nes, question_class, mapping_dict):\n",
    "    f1 = open('ne_%s.txt' % question_class, 'w+')\n",
    "    for idx, doc in enumerate(nes):\n",
    "        f1.write('%s %s\\n' % (mapping_dict[idx], ' '.join(doc)))\n",
    "    f1.close()    \n",
    "\n",
    "def wmd_compute(x1, x2):\n",
    "    return wv.wmdistance(x1, x2)\n",
    "\n",
    "def analyze_cluster(n_clusters_, labels, corpus, mapping_dict, test_corpus, filename):\n",
    "    num = 0\n",
    "    other_content = []\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        if len(idx_list) > 4:\n",
    "            f1 = open('%s_%s/%dth_cluster.txt' % (filename, question_class, indice_cluster), 'w+')\n",
    "            for idx in idx_list:\n",
    "                f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "            f1.close()\n",
    "        if len(idx_list) < 4:\n",
    "            num = num + len(idx_list)\n",
    "            for idx in idx_list:\n",
    "                other_content.append(corpus[mapping_dict[idx] - 1])\n",
    "    f1 = open('%s_%s/other_cluster.txt' % (filename, question_class), 'w+')\n",
    "    for doc in other_content:\n",
    "        f1.write('%s\\n' % doc)\n",
    "    f1.close()\n",
    "    print num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of comments 445\n"
     ]
    }
   ],
   "source": [
    "question_class = 'q10'\n",
    "fname = '..//data//%s.txt' % question_class\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "print 'length of comments', len(content)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(x1):\n",
    "    x_new = []\n",
    "    for x in x1:\n",
    "        if 'no improvement' in x:\n",
    "            continue\n",
    "        else:\n",
    "            x_new.append(x)\n",
    "    return x_new\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Every dataset is lower cased except for TREC\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \", string) \n",
    "    string = re.sub(r\"\\)\", \" \", string) \n",
    "    string = re.sub(r\"\\?\", \" \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def process_corpus(x1,stopwords):\n",
    "    # split three categories: 1 no improvemnt 2 with noun 3 others\n",
    "    doc_noimprove = []\n",
    "    clean_sen = []\n",
    "    for x in x1:\n",
    "        if 'no improvement' in x or 'respondent' in x:\n",
    "            doc_noimprove.append(x)\n",
    "        else:\n",
    "            sents = sent_tokenize(x)\n",
    "            sen_list = []\n",
    "            for sent in sents:\n",
    "                word_list = nltk.word_tokenize(sent)\n",
    "                word_list = [clean_str(word) for word in word_list if word not in stop_words]\n",
    "                sen_list = sen_list + word_list\n",
    "            sen_list = ' '.join(sen_list)\n",
    "            \n",
    "            clean_sen.append(sen_list.strip())\n",
    "    return clean_sen\n",
    "\n",
    "def should_nn_v(x1, pos_nn, pos_vv):\n",
    "    pos_new = nltk.pos_tag(nltk.word_tokenize(x1))\n",
    "    nn_list = []\n",
    "    vv_list = []\n",
    "    for token in pos_new:\n",
    "        if token[1] in pos_nn: #and token[0] in wv.vocab:\n",
    "            nn_list.append(token[0])\n",
    "        if token[1] in pos_vv:\n",
    "            vv_list.append(token[0])\n",
    "    print nn_list\n",
    "    print vv_list\n",
    "    print x1\n",
    "    \n",
    "nn_corpus = []\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "rule_10_words = ['follow', 'up','call', 'calls', 'give']\n",
    "stop_words = stop_words + rule_10_words\n",
    "new_content = process_corpus(content, stop_words)\n",
    "\n",
    "#pos_nn = ['NN', 'NNS']\n",
    "#pos_vv = ['VB']\n",
    "#print 'no comments', len(doc1)\n",
    "#print 'single sentence without nn', len(doc3)\n",
    "#print 'multi-sentence', len(doc4)\n",
    "#for doc_sing in doc2:\n",
    "    #should_nn_v(doc_sing, pos_nn, pos_vv)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(new_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 50)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "n_topics = 50\n",
    "nmf_data = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5).fit_transform(tfidf)\n",
    "print nmf_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Representation Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "issue mentioned resolve issues customer technician mention situation arrange immediate\n",
      "Topic #1:\n",
      "customer shinrai car days feedback proper money toyota home asked\n",
      "Topic #2:\n",
      "service regarding dealership phone receive till conditions inform toyota things\n",
      "Topic #3:\n",
      "questions feedback know sent asking come car service improvement number\n",
      "Topic #4:\n",
      "parts proper irritating concentrate fallow number contact registered delivery actually\n",
      "Topic #5:\n",
      "clear information dealership good car service customer servicing week feedback\n",
      "Topic #6:\n",
      "job check details needs receive received update times implement feedback\n",
      "Topic #7:\n",
      "relationship car road understand visit came correctly number tell happened\n",
      "Topic #8:\n",
      "problem days vehicle day servicing car solve working delivery condition\n",
      "Topic #9:\n",
      "mail used process dealership available suppose improve tell send getting\n",
      "Topic #10:\n",
      "servicing feedback car customer service time problem ask dealership customers\n",
      "Topic #11:\n",
      "know satisfied period talk information service needs months option comes\n",
      "Topic #12:\n",
      "dealership problem feedback need suggest satisfied received know area clear\n",
      "Topic #13:\n",
      "increase messages maintain send works senior far visit servicing problem\n",
      "Topic #14:\n",
      "problem service vehicle improvement called recognize ask times tell servicing\n",
      "Topic #15:\n",
      "inform period busy feedback free action instead customers carefully visit\n",
      "Topic #16:\n",
      "rectify problems feedback good servicing ask dealership till improve ok\n",
      "Topic #17:\n",
      "develop improved compulsory complete process calling general paper telling till\n",
      "Topic #18:\n",
      "solved recognize service purpose complain possible needs pending issues given\n",
      "Topic #19:\n",
      "got useful feed maintain regarding day half receive faced care\n",
      "Topic #20:\n",
      "vehicle completion customer fallow experience delivery contact 15 taken times\n",
      "Topic #21:\n",
      "feed actual coming quality limited car period check attend suppose\n",
      "Topic #22:\n",
      "improved complaint insurance completed response service happened received making issue\n",
      "Topic #23:\n",
      "needed received calling car change question better service registered completed\n",
      "Topic #24:\n",
      "dealership proper issue limited telling properly condition matter services servicing\n",
      "Topic #25:\n",
      "touch customers check feel mail long running phone use went\n",
      "Topic #26:\n",
      "customer satisfied service number satisfaction free servicing half leave situation\n",
      "Topic #27:\n",
      "email question send mobile use mail post instead services need\n",
      "Topic #28:\n",
      "servicing ask feedback car properly email problem owner customer work\n",
      "Topic #29:\n",
      "time feedback customers servicing problem attend coming contact ask car\n",
      "Topic #30:\n",
      "car regularly customer tell taking facing technicians dealership registered occurs\n",
      "Topic #31:\n",
      "vehicle complaints service period insurance condition answer services customer action\n",
      "Topic #32:\n",
      "feedback confirmation members ok complained time 10 staff sms problem\n",
      "Topic #33:\n",
      "receive servicing feedback tell better executive getting earlier felt query\n",
      "Topic #34:\n",
      "dealership complaint action called knowledge luggage confirmation customer 10 quality\n",
      "Topic #35:\n",
      "improved regular responding calling car actually ups vehicle service days\n",
      "Topic #36:\n",
      "battery feedback car servicing change condition customer till comes updated\n",
      "Topic #37:\n",
      "month enquire car send complaints days problem half kind solve\n",
      "Topic #38:\n",
      "process yes issues say better services called giving using proper\n",
      "Topic #39:\n",
      "tell 24 feedback collect week good hours acknowledge attention getting\n",
      "Topic #40:\n",
      "complaint action dealership wo 10 staff km service time present\n",
      "Topic #41:\n",
      "given things intimation request sit immediate query come maintenance sms\n",
      "Topic #42:\n",
      "luggage return customer complaint given concentrate feedback number change people\n",
      "Topic #43:\n",
      "taken servicing people inform talking felt tell solve using attend\n",
      "Topic #44:\n",
      "feedbacks home return change answer suppose explain general post periodic\n",
      "Topic #45:\n",
      "hours visit called proper pick battery 000 service carefully new\n",
      "Topic #46:\n",
      "collect coming waste different visits time yes servicing self understand\n",
      "Topic #47:\n",
      "concern toyota customers calling nanavati complaints customer time responding people\n",
      "Topic #48:\n",
      "face 48 serviced improve request charges correctly improvement car later\n",
      "Topic #49:\n",
      "message updated instead information proper leave think called like meeting\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics = 50\n",
    "tm_model = LatentDirichletAllocation(n_topics, max_iter=100,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0).fit(tfidf)\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "n_top_words=10\n",
    "tf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(tm_model, tf_feature_names, n_top_words)\n",
    "tm_data = tm_model.transform(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=10, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def print_cluster_docs(model, content):\n",
    "    for topic_idx in enumerate(model.n_clusters):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print '\\n'\n",
    "        for idx, label in enumerate(model_labels_):\n",
    "            if label == topic_idx:\n",
    "                print content[idx]\n",
    "                print '\\n'\n",
    "true_k = 10\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "km.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 5 1 0 0 6 3 9 6 0 8 8 4 8 0 6 9 3 2 8 0 3 5 4 0 1 3 9 1 6 5 4 6 2 0 1 0\n",
      " 8 0 3 1 3 2 7 3 3 3 8 0 0 9 4 2 2 8 3 4 0 3 0 8 1 7 1 0 2 3 5 3 7 1 0 9 5\n",
      " 0 9 0 3 6 5 4 7 3 8 0 6 3 0 3 4 4 5 3 5 6 8 0 1 0 5 1 1 2 1 1 8 0 1 6 1 3\n",
      " 3 2 3 0 0 3 8 3 2 5 8 2 1 5 6 0 8 6 4 4 0 3 0 8 3 2 0 7 0 8 8 9 0 0 0 0 3\n",
      " 6 6 5 0 8 5 7 1 1 5 0 3 8 0 1 6 8 2 6 3 0 3 5 0 8 2 0 3 0 5 8 0 9 3 0 0 9\n",
      " 3 4 1 6 3 0 1 9 1 1 3 0 1 4 3 0 0 9 3 0 3 3 3 1 4 8 4 3 0 3 3 8 4 3 3 5 0\n",
      " 9 1 1 0 9 9 4 3 0 8 1 2 1 5 2 0 1 8 1 7 0 0 3 2 2 3 8 8 9 3 0 3 0 6 0 0 4\n",
      " 0 1 0 1 3 3 3 9 8 3 4 5 0 0 0 1 0 1 0 3 8 1 1 1 9 2 9 4 7 3 1 3 0 1 3 9 3\n",
      " 3 0 1 1 1 9 0 8 6 2 3 0 6 1 3 3 3 1 1 3 3 8 3 4 5 3 9 8 6 8 0 9 3 1 3 5 4\n",
      " 3 4 0 4 0 5 0 4 0 6 0 0 2 0 8 5 0 0 3 4 6 3 0 2 5 1 9 8 7 0 8 3 4 8 0 6 9\n",
      " 3 6 0 9 4 2 3 2 3 1 8 2 0 3 3 3 1 3 8 4 0 3 4 1 0 1 3 1 4 8 3 2 0 3 2 0 3\n",
      " 8 0 1 8 8 3 8 1 8 1 1 3 3 2 1 7 2 3 3]\n"
     ]
    }
   ],
   "source": [
    "print km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
