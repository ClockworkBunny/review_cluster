{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import itertools\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.porter import *\n",
    "#import pyemd\n",
    "#loading word embeddings\n",
    "#wv = KeyedVectors.load_word2vec_format(\"../w2v/w2vemb.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(n_clusters_, labels, corpus, mapping_dict, test_corpus):\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        # print indice_cluster\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        f1 = open('cluster_%s/%dth_cluster.txt' % (question_class, indice_cluster), 'w+')\n",
    "        for idx in idx_list:\n",
    "            f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "    f1.close()\n",
    "\n",
    "\n",
    "def write_ne(nes, question_class, mapping_dict):\n",
    "    f1 = open('ne_%s.txt' % question_class, 'w+')\n",
    "    for idx, doc in enumerate(nes):\n",
    "        f1.write('%s %s\\n' % (mapping_dict[idx], ' '.join(doc)))\n",
    "    f1.close()    \n",
    "\n",
    "def wmd_compute(x1, x2):\n",
    "    return wv.wmdistance(x1, x2)\n",
    "\n",
    "def analyze_cluster(n_clusters_, labels, corpus, mapping_dict, test_corpus, filename):\n",
    "    num = 0\n",
    "    other_content = []\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        if len(idx_list) > 4:\n",
    "            f1 = open('%s_%s/%dth_cluster.txt' % (filename, question_class, indice_cluster), 'w+')\n",
    "            for idx in idx_list:\n",
    "                f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "            f1.close()\n",
    "        if len(idx_list) < 4:\n",
    "            num = num + len(idx_list)\n",
    "            for idx in idx_list:\n",
    "                other_content.append(corpus[mapping_dict[idx] - 1])\n",
    "    f1 = open('%s_%s/other_cluster.txt' % (filename, question_class), 'w+')\n",
    "    for doc in other_content:\n",
    "        f1.write('%s\\n' % doc)\n",
    "    f1.close()\n",
    "    print num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of content 774\n"
     ]
    }
   ],
   "source": [
    "question_class = 'q8'\n",
    "fname = '..//data//%s.txt' % question_class\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "print 'length of content', len(content)\n",
    "f.close()\n",
    "#print content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of new content 1978\n"
     ]
    }
   ],
   "source": [
    "# split comment with multi-sentence into multi-comments\n",
    "content_new = []\n",
    "sent_to_comm_id = []\n",
    "for comment_ind, comment in enumerate(content):\n",
    "    sents = sent_tokenize(comment)\n",
    "    if len(sents) > 1:\n",
    "        for i in range(len(sents)):\n",
    "            content_new.append(sents[i])\n",
    "    else:\n",
    "        content_new.append(comment)\n",
    "    sent_to_comm_id += [comment_ind]*len(sents)\n",
    "    \n",
    "print 'length of new content', len(content_new)\n",
    "# print content_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repair\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "print stemmer.stem(\"repair\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "signle sentence with nn 1103\n",
      "no comments 8\n",
      "single sentence without nn 867\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def rule_q1(sen, ne):\n",
    "    clean_ne = list(set(ne))\n",
    "#     remove_words = [\"improv\", \"custom\", \"servic\", \"peopl\",\"person\",\"facil\",\"avail\",\"good\",\\\n",
    "#                     \"center\",\"centr\",\"car\", \"dealership\", \"vehicl\", \"toyota\", \"problem\",\"work\", \"much\",\\\n",
    "#                    \"thing\", \"possibl\",\"need\"]   #stemmed\n",
    "    remove_words = ['custom','hour','minut','time','car', 'vehicl','servic', 'work', 'day','week','dealership',\\\n",
    "                   'center', 'toyota', 'appoint', 'pm', 'problem', 'period', 'part', 'morn','improv',\\\n",
    "                   'hr', 'peopl','person', 'lot', 'need','even', 'henc', 'manag','area', 'thing', 'till',\\\n",
    "                   'place', 'job', 'compani','today', 'call', 'clock', 'o\\'clock', 'min','chang',\\\n",
    "                   'half', 'number']  #stemmed\n",
    "    clean_ne = [word for word in clean_ne if word not in remove_words]\n",
    "    \n",
    "    save_words = ['commit', 'inform', 'delay', 'wash','wast','repair', 'fix', 'bill','mention','charg',\\\n",
    "                 'exact','extra', 'mechan','clean', 'spare','coordin', 'commun', 'far', 'return','deliv']  #stemmed\n",
    "    clean_ne = clean_ne + [stemmer.stem(word) for word in sen.split() if stemmer.stem(word) in save_words]\n",
    "    \n",
    "    # rules to merge keywords:\n",
    "    if 'delay' in clean_ne:\n",
    "        clean_ne[clean_ne.index('delay')] = 'commit'\n",
    "    if 'extra' in clean_ne:\n",
    "        clean_ne[clean_ne.index('extra')] = 'commit'\n",
    "        \n",
    "    if 'mention' in clean_ne:\n",
    "        clean_ne[clean_ne.index('mention')] = 'inform'\n",
    "    if 'coordin' in clean_ne:\n",
    "        clean_ne[clean_ne.index('coordin')] = 'inform'\n",
    "    if 'commun' in clean_ne:\n",
    "        clean_ne[clean_ne.index('commun')] = 'inform'\n",
    "    if 'how much time' in sen:\n",
    "        clean_ne.append('inform')\n",
    "        \n",
    "    if 'return' in clean_ne:\n",
    "        clean_ne[clean_ne.index('return')] = 'deliveri'\n",
    "    if 'deliv' in clean_ne:\n",
    "        clean_ne[clean_ne.index('deliv')] = 'deliveri'\n",
    "        \n",
    "    if 'accessori' in clean_ne:\n",
    "        clean_ne[clean_ne.index('accessori')] = 'spare'\n",
    "        \n",
    "    if 'advisor' in clean_ne:\n",
    "        clean_ne[clean_ne.index('advisor')] = 'staff'\n",
    "    if 'laor' in clean_ne:\n",
    "        clean_ne[clean_ne.index('laor')] = 'staff'\n",
    "    if 'manpow' in clean_ne:\n",
    "        clean_ne[clean_ne.index('manpow')] = 'staff'\n",
    "    if 'man power' in sen:\n",
    "        clean_ne.append('staff')\n",
    "    if 'serviceman' in clean_ne:\n",
    "        clean_ne[clean_ne.index('serviceman')] = 'staff'\n",
    "    if 'worker' in clean_ne:\n",
    "        clean_ne[clean_ne.index('worker')] = 'staff'\n",
    "    if 'mechan' in clean_ne:\n",
    "        clean_ne[clean_ne.index('mechan')] = 'staff'\n",
    "    \n",
    "        \n",
    "    if 'km' in clean_ne:\n",
    "        clean_ne[clean_ne.index('km')] = 'distanc'\n",
    "    if 'far' in clean_ne:\n",
    "        clean_ne[clean_ne.index('far')] = 'distanc' \n",
    "    \n",
    "    if 'fix' in clean_ne:\n",
    "        if 'appointment' not in sen:\n",
    "            clean_ne[clean_ne.index('fix')] = 'repair'\n",
    "        else:\n",
    "            clean_ne[clean_ne.index('fix')] = 'commit'\n",
    "        \n",
    "        \n",
    "    if 'wast' in clean_ne:\n",
    "        clean_ne[clean_ne.index('wast')] = 'wait' \n",
    "    if 'speed' in clean_ne:\n",
    "        clean_ne[clean_ne.index('speed')] = 'wait'\n",
    "    \n",
    "    clean_ne = list(set(clean_ne))\n",
    "    return clean_ne\n",
    "\n",
    "\n",
    "def process_corpus(x1, pos_tags, general_stop,sent_ind): #new process_corpus func.===============================\n",
    "    # split three categories: 1 no improvemnt 2 with noun 3 others\n",
    "    doc_noimprove = []\n",
    "    doc_nn = []\n",
    "    nn_extracted = []\n",
    "    doc_other = []\n",
    "    r_ind = []\n",
    "    for ind,x in enumerate(x1):\n",
    "        if 'no improvement' in x:\n",
    "            doc_noimprove.append(x)\n",
    "            r_ind.append(ind)\n",
    "        else:\n",
    "            nn_list = []\n",
    "            sen = x\n",
    "            pos_new = nltk.pos_tag(nltk.word_tokenize(sen))\n",
    "            for token in pos_new:\n",
    "                if token[1] in pos_tags and not token[0] in general_stop:\n",
    "                    nn_list.append(token[0])\n",
    "            nn_list = [stemmer.stem(word) for word in nn_list] #stemming\n",
    "            nn_list = rule_q1(sen, nn_list) # apply rule\n",
    "                    \n",
    "            if nn_list != []:\n",
    "                nn_extracted.append(nn_list)\n",
    "                doc_nn.append(sen)\n",
    "            else:\n",
    "                doc_other.append(sen)\n",
    "                r_ind.append(ind)\n",
    "    sent_ind = [i for j, i in enumerate(sent_ind) if j not in r_ind]\n",
    "    return doc_noimprove, [doc_nn, nn_extracted], doc_other, sent_ind\n",
    "\n",
    "nn_corpus = []\n",
    "\n",
    "# English stop words lists\n",
    "stop_words = stopwords.words('english')\n",
    "punctuation_list = [unicode(i) for i in string.punctuation]\n",
    "for punctuation in punctuation_list:\n",
    "    stop_words.append(punctuation)\n",
    "\n",
    "pos_tags = ['NN', 'NNS']\n",
    "\n",
    "doc1, doc2, doc3, sent_comm_ind = process_corpus(content_new, pos_tags, stop_words, sent_to_comm_id)\n",
    "\n",
    "doc_nn, nn_extracted = doc2[0], doc2[1]\n",
    "print 'signle sentence with nn', len(doc_nn)\n",
    "print 'no comments', len(doc1)\n",
    "print 'single sentence without nn', len(doc3)\n",
    "#print nn_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'deliveri': 239, u'commit': 219, 'staff': 137, 'inform': 73, 'wait': 45, 'repair': 33, 'wash': 25, 'distanc': 25, u'deliv': 18, u'bill': 18, u'charg': 16, 'exact': 15, u'issu': 13, 'spare': 11, 'oil': 11, 'card': 9, u'facil': 9, 'clean': 9, u'increas': 9, u'process': 8, 'return': 8, 'care': 8, 'tomorrow': 8, 'hand': 8, u'attent': 7, 'delay': 7, 'labor': 7, 'space': 7, 'showroom': 7, 'power': 7, 'paper': 6, 'lunch': 6, u'advanc': 6, 'reason': 6, u'centr': 6, 'complaint': 6, 'fast': 6, 'way': 6, u'depart': 5, 'date': 5, 'side': 5, u'qualiti': 5, u'comput': 5, u'plan': 5, u'book': 5, 'visit': 5, 'wise': 5, 'month': 4, u'mechan': 4, 'water': 4, u'valu': 4, u'commun': 4, u'purpos': 4, 'man': 4, u'sm': 4, 'feedback': 4, 'fix': 4, 'name': 4, u'anyth': 4, u'capac': 4, 'owner': 4, u'citi': 4, 'system': 4, u'noth': 4, u'insur': 4, u'express': 4, 'frame': 4, u'respons': 4, u'someth': 4, 'ramp': 4, u'suggest': 4, u'everyth': 4, 'home': 4, u'machin': 4, 'technician': 4, 'rush': 4, 'money': 4, 'dealer': 4, 'lack': 3, 'sound': 3, u'pass': 3, u'busi': 3, u'hurri': 3, 'phone': 3, 'tax': 3, 'gate': 3, 'counter': 3, 'bank': 3, 'drop': 3, 'year': 3, u'model': 3, u'messag': 3, 'client': 3, u'payment': 3, 'fine': 3, u'locat': 3, 'screen': 3, 'point': 3, 'supervisor': 3, u'offic': 3, 'workshop': 3, 'check': 3, 'member': 3, 'wheel': 3, u'formal': 3, 'start': 3, u'emerg': 3, u'middl': 2, u'consum': 2, 'lanson': 2, 'boy': 2, 'queue': 2, 'opinion': 2, u'prefer': 2, 'total': 2, u'mobil': 2, 'ward': 2, u'accid': 2, u'word': 2, u'hous': 2, u'employe': 2, 'claim': 2, 'get': 2, 'sunday': 2, u'product': 2, u'outsid': 2, 'mam': 2, 'maintain': 2, 'order': 2, 'auto': 2, 'break': 2, u'timelin': 2, u'procedur': 2, 'millennium': 2, u'afterward': 2, u'wast': 2, u'prioriti': 2, u'christma': 2, u'paint': 2, u'nobodi': 2, u'serv': 2, u'conveni': 2, u'provid': 2, 'pre': 2, u'mint': 2, u'engin': 2, u'excus': 2, 'drive': 2, 'fact': 2, 'shop': 2, u'permiss': 2, u'solut': 2, u'satisfact': 2, u'statu': 2, 'pickup': 2, 'c': 2, u'committe': 2, u'schedul': 2, u'imag': 2, u'coordin': 2, 'case': 2, u'everyon': 2, 'driver': 2, u'someon': 2, u'exampl': 2, 'left': 2, 'load': 2, u'remind': 2, u'mistak': 2, 'confirm': 2, 'night': 2, 'ok': 2, 'afternoon': 2, u'avail': 2, 'offer': 2, 'link': 2, 'type': 2, u'record': 2, 'limit': 2, u'decreas': 2, 'saturday': 2, u'other': 2, 'test': 2, 'e': 2, 'rs': 2, u'queri': 1, 'go': 1, 'follow': 1, 'disk': 1, 'depend': 1, 'jaipur': 1, 'tv': 1, 'thursday': 1, u'program': 1, u'chequ': 1, u'govern': 1, 'level': 1, 'wednesday': 1, u'verif': 1, 'quick': 1, 'madurai': 1, 'harsha': 1, u'pressur': 1, u'design': 1, u'45minut': 1, u'supervis': 1, u'insid': 1, u'compar': 1, 'turnaround': 1, u'kilomet': 1, 'public': 1, u'60minut': 1, u'gener': 1, u'entertain': 1, 'china': 1, 'box': 1, u'search': 1, 'shift': 1, u'technolog': 1, u'outskirt': 1, 'sharayu': 1, 'amount': 1, 'extra': 1, 'servicemen': 1, 'vit': 1, 'use': 1, u'regist': 1, u'everybodi': 1, u'basi': 1, 'wiper': 1, 'paperwork': 1, u'notic': 1, u'sens': 1, 'hold': 1, 'town': 1, u'account': 1, u'sometim': 1, 'cat': 1, 'cab': 1, u'thig': 1, u'trichi': 1, 'minimum': 1, 'end': 1, 'clint': 1, u'travel': 1, u'write': 1, u'anyon': 1, 'tile': 1, 'stock': 1, 'okay': 1, u'villag': 1, 'attempt': 1, 'third': 1, u'crowded': 1, 'light': 1, u'allot': 1, 'enter': 1, 'innova': 1, 'approx': 1, 'japan': 1, u'bangalor': 1, u'famili': 1, 'outer': 1, u'sentenc': 1, 'window': 1, u'infrastructur': 1, u'farmer': 1, u'food': 1, 'workout': 1, u'verifi': 1, 'dainpaint': 1, u'mean': 1, 'em': 1, 'eg': 1, 'idea': 1, u'expect': 1, 'topsel': 1, 'bima': 1, 'diesel': 1, u'profil': 1, u'navig': 1, 'team': 1, 'borivali': 1, 'turn': 1, 'w': 1, 'bumper': 1, 'think': 1, 'rane': 1, u'feel': 1, u'vechil': 1, 'sheet': 1, 'station': 1, 'scheme': 1, 'friend': 1, 'gear': 1, 'relationship': 1, u'toot': 1, 'target': 1, u'whenev': 1, u'balanc': 1, u'posit': 1, u'arrang': 1, u'toward': 1, 'pudukkottai': 1, 'mine': 1, 'option': 1, u'recharg': 1, 'courier': 1, u'accuraci': 1, 'take': 1, u'cargo': 1, 'track': 1, 'price': 1, u'regular': 1, 'letter': 1, 'later': 1, 'request': 1, u'quantiti': 1, 'corner': 1, u'rough': 1, 'line': 1, u'penalti': 1, u'nois': 1, u'stand': 1, u'suppos': 1, u'factor': 1, u'requir': 1, u'cleanli': 1, u'respond': 1, 'madam': 1, u'set': 1, 'fair': 1, u'knowledg': 1, 'buddha': 1, u'concern': 1, 'wire': 1, u'inconveni': 1, 'espirit': 1, 'sonpur': 1, u'boss': 1, 'key': 1, u'shortag': 1, u'advertis': 1, 'come': 1, u'bodi': 1, u'lass': 1, 'etc': 1, 'comer': 1, u'comment': 1, u'author': 1, 'loan': 1, u'arriv': 1, u'scervix': 1, u'batteri': 1, 'bhubaneswar': 1, 'thrice': 1, 'amana': 1, 'gap': 1, 'lift': 1, 'understand': 1, u'align': 1, 'cash': 1, u'velacheri': 1, u'durat': 1, 'kadloor': 1, 'behavior': 1, 'dindivanam': 1, u'timefram': 1, 'puncher': 1, u'garag': 1, 'site': 1, u'bias': 1, 'trial': 1, 'rajendra': 1, u'complet': 1, u'assist': 1, u'coffe': 1, 'moment': 1, u'wherev': 1, 'tune': 1, u'squar': 1, u'weld': 1, u'mention': 1, u'expens': 1, u'programm': 1, 'breaker': 1, u'execut': 1, 'photo': 1, u'ordin': 1, 'rest': 1, 'ballangir': 1, u'aspect': 1, u'handov': 1, u'lakh': 1, 'anaamalai': 1, u'pondicherri': 1, 'citizen': 1, 'save': 1, u'disciplin': 1, 'delhi': 1, u'ladi': 1, 'traffic': 1, 'bit': 1, 'server': 1, 'annamalai': 1, u'motor': 1, 'deal': 1, 'aliment': 1, u'rever': 1, 'proper': 1, u'transport': 1, u'raibar': 1, u'leav': 1, u'condit': 1, 'koyambedu': 1, u'confer': 1, 'discount': 1, 'step': 1, 'kopar': 1, 'stage': 1, 'cuttack': 1, 'backup': 1, 'tome': 1, 'road': 1, u'effici': 1, 'son': 1, u'assess': 1, u'suffici': 1, 'head': 1, 'receipt': 1, 'brand': 1, u'crane': 1, u'immedi': 1, 'display': 1, 'ex': 1, 'ac': 1, 'mirror': 1, u'detail': 1, 'rubber': 1, u'branch': 1, 'monday': 1, u'reduc': 1, 'faster': 1, 'jmk': 1, 'rule': 1, u'koyun': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def df_count(x1):\n",
    "    text = []\n",
    "    for ab in x1:\n",
    "        text = text + ab\n",
    "    df = Counter(text)\n",
    "    return df\n",
    "df = df_count(nn_extracted)\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use one suitable aspect to represent each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'deliveri': 239, u'commit': 166, 'staff': 125, 'inform': 48, 'wait': 40, 'repair': 28, 'distanc': 21, 'wash': 21, u'charg': 14, u'issu': 12, u'bill': 11, 'spare': 9, 'exact': 8, u'facil': 8, 'oil': 8, 'clean': 6, 'paper': 5, 'lunch': 5, u'qualiti': 5, 'care': 5, u'process': 5, 'complaint': 4, 'space': 4, u'advanc': 4, 'reason': 4, 'card': 4, 'tomorrow': 4, 'fast': 4, 'wise': 4, u'comput': 4, u'centr': 4, u'suggest': 4, u'book': 4, u'noth': 4, 'labor': 3, 'name': 3, u'valu': 3, u'respons': 3, u'someth': 3, 'showroom': 3, 'power': 3, 'screen': 3, 'ramp': 3, 'visit': 3, 'start': 3, u'purpos': 3, 'counter': 3, 'year': 3, 'dealer': 3, u'insur': 3, 'shop': 2, 'feedback': 2, 'money': 2, u'month': 2, 'fine': 2, u'busi': 2, 'rs': 2, u'middl': 2, u'other': 2, u'pass': 2, u'express': 2, u'anyth': 2, 'side': 2, 'home': 2, 'water': 2, 'c': 2, 'ok': 2, u'outsid': 2, u'capac': 2, u'messag': 2, 'owner': 2, u'prioriti': 2, u'christma': 2, u'citi': 2, 'system': 2, 'workshop': 2, 'way': 2, u'hurri': 2, 'phone': 2, 'technician': 2, 'e': 2, 'sound': 2, 'rush': 2, u'provid': 2, u'exampl': 2, u'record': 2, u'mint': 2, 'gate': 2, 'afternoon': 2, u'everyth': 2, u'model': 2, 'opinion': 2, u'consum': 1, u'aspect': 1, 'go': 1, 'saturday': 1, u'rough': 1, u'accuraci': 1, 'ward': 1, u'locat': 1, u'sentenc': 1, 'jaipur': 1, u'solut': 1, u'pondicherri': 1, u'suppos': 1, u'factor': 1, 'save': 1, u'serv': 1, u'satisfact': 1, u'food': 1, u'prefer': 1, 'break': 1, u'govern': 1, u'increas': 1, u'word': 1, 'bit': 1, u'paint': 1, 'bank': 1, u'formal': 1, u'bangalor': 1, 'drop': 1, u'cleanli': 1, 'night': 1, 'quick': 1, 'stock': 1, 'depend': 1, 'deal': 1, 'okay': 1, u'procedur': 1, 'maintain': 1, u'pressur': 1, 'lanson': 1, 'topsel': 1, u'transport': 1, u'concern': 1, u'statu': 1, u'insid': 1, u'mobil': 1, u'raibar': 1, u'profil': 1, 'turnaround': 1, u'committe': 1, u'confer': 1, u'entertain': 1, 'key': 1, 'dainpaint': 1, u'shortag': 1, u'bodi': 1, u'lass': 1, u'technolog': 1, 'cuttack': 1, u'offic': 1, u'stand': 1, 'bumper': 1, 'china': 1, 'point': 1, u'arriv': 1, u'assess': 1, 'vit': 1, 'supervisor': 1, 'total': 1, 'use': 1, 'rajendra': 1, u'everybodi': 1, 'tune': 1, u'immedi': 1, 'type': 1, 'friend': 1, 'head': 1, 'brand': 1, 'link': 1, u'crane': 1, 'scheme': 1, 'target': 1, u'whenev': 1, 'cash': 1, u'posit': 1, u'arrang': 1, u'everyon': 1, 'ex': 1, 'pre': 1, 'tax': 1, 'site': 1, 'jmk': 1, u'bias': 1, u'decreas': 1, 'station': 1, u'scervix': 1, u'travel': 1, u'detail': 1, 'client': 1, u'member': 1, 'sunday': 1, u'anyon': 1, u'branch': 1, 'auto': 1, 'test': 1, 'w': 1, 'driver': 1, u'someon': 1, 'hand': 1, 'moment': 1, u'plan': 1, u'attent': 1, 'date': 1, u'wherev': 1, 'mam': 1, u'reduc': 1, 'breaker': 1, u'weld': 1, 'faster': 1, 'third': 1, u'crowded': 1, 'light': 1, 'later': 1, 'rule': 1, u'emerg': 1, u'expens': 1, u'sm': 1, 'order': 1, 'fact': 1, 'left': 1})\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import os\n",
    "def filter_rule_q1(doc, xth, test_corpus, original_corpus = doc_nn):\n",
    "    apply_rule = False\n",
    "    # print original_corpus[xth]\n",
    "    # rules (priority from high to low)\n",
    "         \n",
    "    return apply_rule\n",
    "    \n",
    "def filter_ne(test_corpus, df):  # assuming each review contain one aspect\n",
    "    for xth, doc in enumerate(test_corpus):\n",
    "        if len(doc)>1:\n",
    "            apply_rule = filter_rule_q1(doc, xth, test_corpus)\n",
    "            if not apply_rule:\n",
    "                df_words = [df[word] for word in doc]\n",
    "                idx =  heapq.nlargest(1, xrange(len(df_words)), key=df_words.__getitem__)\n",
    "                test_corpus[xth] = [doc[ith] for ith in idx]\n",
    "    return test_corpus\n",
    "nn_clean = filter_ne(nn_extracted, df)\n",
    "df = df_count(nn_clean)\n",
    "dict_map = dict(df.most_common())\n",
    "print df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Decide main cluster and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "majot list:\n",
      "['distanc', 'staff', 'exact', 'spare', 'wait', u'commit', 'repair', 'wash', u'facil', 'deliveri', u'bill', 'oil', u'charg', u'issu', 'inform', 'clean']\n",
      "num of clustered: 764\n",
      "num of others: 339\n",
      "comment ratio 65.1436031332\n"
     ]
    }
   ],
   "source": [
    "def write_file(corpus, idx_list, word):\n",
    "    if not os.path.isdir(\"cluster/%s\" %word):\n",
    "            os.mkdir(\"cluster/%s\" %word)\n",
    "    f1 = open('cluster/%s/%s_comment.txt' % (word, word), 'w+')\n",
    "    for idx in idx_list:\n",
    "        f1.write('%s\\n' %corpus[idx])\n",
    "    f1.close()\n",
    "    \n",
    "    \n",
    "def main_category(df_list, nn_clean, corpus,sc_ind):\n",
    "    if (not os.path.isdir(\"cluster\")):\n",
    "        os.mkdir(\"cluster\")\n",
    "    name_list = {}\n",
    "    clustered_index = []\n",
    "    scidx_set = []\n",
    "    major_list = [word for word in df_list if df_list[word]>5]\n",
    "    print \"majot list:\\n\", major_list\n",
    "    for word in major_list:\n",
    "        idx_set = []\n",
    "        for idx, doc in enumerate(nn_clean):\n",
    "            if word in doc:\n",
    "                idx_set.append(idx)\n",
    "        write_file(doc_nn, idx_set, word)\n",
    "        name_list[word] = idx_set\n",
    "        clustered_index = clustered_index + idx_set\n",
    "        scidx_set += [sc_ind[i] for i in idx_set]\n",
    "    unclustered_index = [x for x in range(len(nn_clean)) if x not in clustered_index]\n",
    "    print \"num of clustered:\",len(clustered_index)\n",
    "    print \"num of others:\", len(unclustered_index)\n",
    "    name_list['others'] = unclustered_index\n",
    "    write_file(doc_nn, unclustered_index, 'others')\n",
    "    return name_list, len(set(scidx_set))\n",
    "\n",
    "\n",
    "name_list, nb_comm = main_category(dict_map, nn_clean,doc_nn, sent_comm_ind)\n",
    "\n",
    "print 'comment ratio', float(nb_comm)/(len(content)-len(doc1))*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
