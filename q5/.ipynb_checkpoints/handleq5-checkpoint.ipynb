{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import itertools\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#import pyemd\n",
    "#loading word embeddings\n",
    "#wv = KeyedVectors.load_word2vec_format(\"../w2v/w2vemb.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(n_clusters_, labels, corpus, mapping_dict, test_corpus):\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        # print indice_cluster\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        f1 = open('cluster_%s/%dth_cluster.txt' % (question_class, indice_cluster), 'w+')\n",
    "        for idx in idx_list:\n",
    "            f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "    f1.close()\n",
    "\n",
    "\n",
    "def write_ne(nes, question_class, mapping_dict):\n",
    "    f1 = open('ne_%s.txt' % question_class, 'w+')\n",
    "    for idx, doc in enumerate(nes):\n",
    "        f1.write('%s %s\\n' % (mapping_dict[idx], ' '.join(doc)))\n",
    "    f1.close()    \n",
    "\n",
    "def wmd_compute(x1, x2):\n",
    "    return wv.wmdistance(x1, x2)\n",
    "\n",
    "def analyze_cluster(n_clusters_, labels, corpus, mapping_dict, test_corpus, filename):\n",
    "    num = 0\n",
    "    other_content = []\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        if len(idx_list) > 4:\n",
    "            f1 = open('%s_%s/%dth_cluster.txt' % (filename, question_class, indice_cluster), 'w+')\n",
    "            for idx in idx_list:\n",
    "                f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "            f1.close()\n",
    "        if len(idx_list) < 4:\n",
    "            num = num + len(idx_list)\n",
    "            for idx in idx_list:\n",
    "                other_content.append(corpus[mapping_dict[idx] - 1])\n",
    "    f1 = open('%s_%s/other_cluster.txt' % (filename, question_class), 'w+')\n",
    "    for doc in other_content:\n",
    "        f1.write('%s\\n' % doc)\n",
    "    f1.close()\n",
    "    print num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of comments 463\n"
     ]
    }
   ],
   "source": [
    "question_class = 'q5'\n",
    "fname = '..//data//%s.txt' % question_class\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "print 'length of comments', len(content)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n",
      "signle sentence with nn 216\n",
      "no comments 13\n",
      "single sentence without nn 2\n",
      "multi-sentence 232\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def rule_q7(sen, ne):\n",
    "    clean_ne = list(set(ne))\n",
    "    remove_words = ['customer', 'customers', 'waiting', 'facility', 'facilities', 'dealership', 'toyota', 'improvement']\n",
    "    clean_ne = [word for word in clean_ne if word not in remove_words]\n",
    "    if len(clean_ne) > 1 and 'area' in clean_ne:\n",
    "        clean_ne.remove('area')\n",
    "    if len(clean_ne) < 2 and 'area' in clean_ne:\n",
    "        clean_ne[clean_ne.index('area')] = 'space'\n",
    "    if 'place' in clean_ne:\n",
    "        clean_ne[clean_ne.index('place')] = 'space'\n",
    "    return clean_ne\n",
    "\n",
    "def clean_corpus(x1):\n",
    "    x_new = []\n",
    "    for x in x1:\n",
    "        if 'no improvement' in x:\n",
    "            continue\n",
    "        else:\n",
    "            x_new.append(x)\n",
    "    return x_new\n",
    "\n",
    "def process_corpus(x1, pos_tags, general_stop):\n",
    "    # split three categories: 1 no improvemnt 2 with noun 3 others\n",
    "    doc_noimprove = []\n",
    "    doc_nn = []\n",
    "    nn_extracted = []\n",
    "    doc_other = []\n",
    "    doc_multi = []\n",
    "    for x in x1:\n",
    "        if 'no improvement' in x or 'respondent' in x:\n",
    "            doc_noimprove.append(x)\n",
    "        else:\n",
    "            sents = sent_tokenize(x)\n",
    "            if len(sents) > 1:\n",
    "                doc_multi.append(x)\n",
    "            else:\n",
    "                nn_list = []\n",
    "                sen = sents[0]\n",
    "                pos_new = nltk.pos_tag(nltk.word_tokenize(sen))\n",
    "                for token in pos_new:\n",
    "                    if token[1] in pos_tags and not token[0] in general_stop: #and token[0] in wv.vocab:\n",
    "                        nn_list.append(token[0])\n",
    "                    nn_list = rule_q7(sen, nn_list)\n",
    "                if nn_list != []:\n",
    "                    nn_extracted.append(nn_list)\n",
    "                    doc_nn.append(sen)\n",
    "                else:\n",
    "                    doc_other.append(sen)\n",
    "    return doc_noimprove, [doc_nn, nn_extracted], doc_other, doc_multi\n",
    "        \n",
    "nn_corpus = []\n",
    "\n",
    "# English stop words lists\n",
    "stop_words = stopwords.words('english')\n",
    "punctuation_list = [unicode(i) for i in string.punctuation]\n",
    "\n",
    "for punctuation in punctuation_list:\n",
    "    stop_words.append(punctuation)\n",
    "\n",
    "pos_tags = ['NN', 'NNS']\n",
    "print len(content)\n",
    "doc1, doc2, doc3, doc4 = process_corpus(content, pos_tags, stop_words)\n",
    "\n",
    "doc_nn, nn_extracted = doc2[0], doc2[1]\n",
    "print 'signle sentence with nn', len(doc_nn)\n",
    "print 'no comments', len(doc1)\n",
    "print 'single sentence without nn', len(doc3)\n",
    "print 'multi-sentence', len(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'space': 108, 'time': 25, 'tea': 19, 'coffee': 14, 'water': 14, 'arrangement': 13, 'service': 12, 'ac': 12, 'tv': 10, 'center': 10, 'car': 9, 'staff': 8, 'room': 8, 'magazine': 7, 'servicing': 6, 'vehicle': 6, 'washroom': 4, 'seating': 4, 'wait': 4, 'owner': 4, 'lot': 4, 'something': 4, 'driver': 4, 'summer': 3, 'increase': 3, 'refreshment': 3, 'television': 3, 'condition': 3, 'chairs': 3, 'sofa': 3, 'entertainment': 3, 'noise': 3, 'wi': 3, 'hours': 3, 'camera': 3, 'newspaper': 3, 'compare': 3, 'snacks': 3, 'needs': 3, 'response': 3, 'person': 3, 'drivers': 3, 'paper': 2, 'chair': 2, 'bit': 2, 'arrange': 2, 'food': 2, 'cctv': 2, 'lunch': 2, 'drinking': 2, 'morning': 2, 'people': 2, 'seat': 2, 'canteen': 2, 'capacity': 2, 'news': 2, 'anything': 2, 'games': 2, 'family': 2, 'cleanness': 2, 'way': 2, 'season': 2, 'rush': 2, 'air': 2, 'toilet': 2, 'process': 2, 'need': 2, 'cleaning': 2, 'services': 2, 'fi': 2, 'shop': 1, 'cabins': 1, 'executive': 1, 'rest': 1, 'manager': 1, 'comfortless': 1, 'rooms': 1, 'seats': 1, 'couch': 1, 'issues': 1, 'disturbance': 1, 'fit': 1, 'cup': 1, 'supply': 1, 'lake': 1, 'entitlement': 1, 'location': 1, 'travelling': 1, 'cots': 1, 'number': 1, 'fan': 1, 'fall': 1, 'cage': 1, 'bas': 1, 'spaces': 1, 'queries': 1, 'particles': 1, 'fans': 1, 'hour': 1, 'dustbin': 1, 'computer': 1, 'lanson': 1, 'pass': 1, 'saucer': 1, 'cleanliness': 1, 'outside': 1, 'machine': 1, 'neat': 1, 'receiving': 1, 'issue': 1, 'terms': 1, 'power': 1, 'job': 1, 'contort': 1, 'others': 1, 'carom': 1, 'change': 1, 'box': 1, 'launch': 1, 'showrooms': 1, 'host': 1, 'thing': 1, 'afternoon': 1, 'hence': 1, 'hospitality': 1, 'changes': 1, 'ladies': 1, 'aria': 1, 'point': 1, 'ground': 1, 'cofee': 1, 'advisor': 1, 'dust': 1, 'magazines': 1, 'sunlight': 1, 'table': 1, 'eminence': 1, 'quality': 1, 'size': 1, 'consuming': 1, 'croudy': 1, 'billing': 1, 'waits': 1, 'harsha': 1, 'system': 1, 'fast': 1, 'class': 1, 'care': 1, 'cabin': 1, 'company': 1, 'delivery': 1, 'look': 1, 'places': 1, 'comics': 1, 'setup': 1, 'work': 1, 'cash': 1, 'pace': 1, 'limit': 1, 'dirty': 1, 'coffie': 1, 'compulsory': 1, 'kolhapur': 1, 'mechanics': 1, 'wifi': 1, 'tap': 1, 'floor': 1, 'suggestions': 1, 'foods': 1, 'arrangements': 1, 'conditioner': 1, 'breakfast': 1, 'etc': 1, 'things': 1, 'discussion': 1, 'comfort': 1, 'freshener': 1, 'lounge': 1, 'till': 1, 'take': 1, 'drinks': 1, 'channel': 1, 'waters': 1, 'drink': 1, 'kit': 1, 'tries': 1, 'sheets': 1, 'thora': 1, 'counter': 1, 'boor': 1, 'points': 1, 'clean': 1, 'rupees': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def df_count(x1):\n",
    "    # split three categories: 1 no improvemnt 2 with noun 3 others\n",
    "    text = []\n",
    "    for ab in x1:\n",
    "        text = text + ab\n",
    "    df = Counter(text)\n",
    "    return df\n",
    "df = df_count(nn_extracted)\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'space': 107, 'time': 20, 'tea': 12, 'room': 7, 'ac': 6, u'servic': 6, 'arrangement': 5, 'tv': 4, u'arrang': 4, 'washroom': 3, 'water': 3, 'staff': 2, u'driver': 2, u'coffe': 2, 'tap': 1, 'snacks': 1, u'process': 1, 'executive': 1, 'suggestions': 1, u'seat': 1, 'foods': 1, 'service': 1, 'comfortless': 1, 'canteen': 1, u'comput': 1, 'breakfast': 1, 'chair': 1, u'locat': 1, u'televis': 1, u'compar': 1, 'comfort': 1, u'loung': 1, 'lot': 1, u'morn': 1, 'receiving': 1, 'coffee': 1, u'ba': 1, 'food': 1, 'queries': 1, 'rush': 1, 'host': 1, 'wi': 1, 'television': 1, 'chairs': 1, 'car': 1, 'cleanliness': 1, 'arrangements': 1})\n",
      "<type 'list'>\n",
      "['ac', 'tea', 'space', 'tv', 'arrangement', 'room', u'servic', 'time', u'arrang']\n",
      "[1, 2, 3, 4, 8, 11, 13, 14, 18, 25, 27, 33, 35, 40, 41, 42, 49, 50, 53, 55, 59, 62, 65, 68, 69, 78, 81, 82, 85, 86, 87, 90, 91, 94, 95, 97, 98, 102, 103, 104, 109, 111, 112, 113, 115, 117, 118, 119, 120, 121, 122, 123, 124, 126, 128, 130, 132, 133, 135, 136, 137, 139, 140, 141, 142, 143, 145, 146, 147, 148, 149, 150, 151, 153, 154, 156, 158, 159, 160, 161, 162, 163, 166, 167, 168, 175, 177, 178, 182, 185, 186, 189, 190, 192, 193, 194, 197, 198, 200, 202, 204, 206, 208, 209, 211, 212, 213]\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import os\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "def filter_ne(test_corpus, df):  # assuming each review contain one aspect\n",
    "    for xth, doc in enumerate(test_corpus):\n",
    "        if len(doc)>1:\n",
    "            df_words = [df[word] for word in doc]\n",
    "            idx =  heapq.nlargest(1, xrange(len(df_words)), key=df_words.__getitem__)\n",
    "            test_corpus[xth] = [stemmer.stem(doc[ith]) for ith in idx]\n",
    "    return test_corpus\n",
    "nn_clean = filter_ne(nn_extracted, df)\n",
    "df = df_count(nn_clean)\n",
    "dict_map = dict(df.most_common())\n",
    "print df\n",
    "print type(nn_clean[0])\n",
    "\n",
    "def write_file(corpus, idx_list, word):\n",
    "    f1 = open('cluster/%s/%s_comment.txt' % (word, word), 'w+')\n",
    "    for idx in idx_list:\n",
    "        f1.write('%s\\n' %corpus[idx])\n",
    "    f1.close()\n",
    "    \n",
    "    \n",
    "def main_category(df_list, nn_clean, corpus):\n",
    "    os.mkdir(\"cluster\")\n",
    "    name_list = {}\n",
    "    major_list = [word for word in df_list if df_list[word]>3]\n",
    "    print major_list\n",
    "    for word in major_list:\n",
    "        os.mkdir(\"cluster/%s\" %word)\n",
    "        idx_set = []\n",
    "        for idx, doc in enumerate(nn_clean):\n",
    "            if word in doc:\n",
    "                idx_set.append(idx)\n",
    "        write_file(doc_nn, idx_set, word)\n",
    "        name_list[word] = idx_set\n",
    "    return name_list\n",
    "\n",
    "name_list = main_category(dict_map, nn_clean,doc_nn)\n",
    "print name_list['space']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Split Large Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from itertools import product\n",
    "def sim_check_word(word1,word2):\n",
    "    syns1 = wn.synsets(word1)\n",
    "    syns2 = wn.synsets(word2)\n",
    "    sims = []\n",
    "    for sense1, sense2 in product(syns1, syns2):\n",
    "        d = wn.path_similarity(sense1, sense2)\n",
    "        sims.append((d))\n",
    "    return max(sims)\n",
    "\n",
    "def sim_check_list(list1,list2):\n",
    "    sims = []\n",
    "    for word in list1:\n",
    "        for word2 in list2:\n",
    "            sims.append(sim_check_word(word, word2))\n",
    "    return max(sims)\n",
    "\n",
    "\n",
    "def l2_extract(corpus, idx_list):\n",
    "    num = 0\n",
    "    other_content = []\n",
    "    adj_batchlist = []\n",
    "    local_content = []\n",
    "    for idx in idx_list:\n",
    "        doc = corpus[idx]\n",
    "        local_content.append(doc)\n",
    "        adj_list = []\n",
    "        for word in doc.split():\n",
    "            try:\n",
    "                tmp = [wn.synsets(word)[hh].pos() for hh in range(len(wn.synsets(word)))] \n",
    "            except IndexError:\n",
    "                tmp = None\n",
    "            if 'a' in tmp:\n",
    "                adj_list.append(word)\n",
    "        adj_batchlist.append(adj_list)\n",
    "    \n",
    "    \n",
    "    return adj_batchlist, local_content\n",
    "\n",
    "tt_list, local_content = l2_extract(doc_nn, name_list['space'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['more', 'sitting'], [], ['comfortable'], ['center'], [], ['improved'], ['proper', 'much'], ['no', 'sitting'], ['small'], ['more', 'clean'], ['little', 'big', 'available'], ['other', 'less'], ['separate', 'available'], ['good'], [], [], [], ['separate'], ['much', 'least'], [], ['away'], [], ['available'], [], ['proper'], ['less'], ['improved', 'small', 'much'], ['small'], ['little', 'more'], ['sitting', 'more'], ['separate'], ['small', 'large', 'comfortable'], ['improved', 'other', 'all'], ['more'], ['visible'], ['small', 'big'], [], ['on'], ['more', 'broad', 'short'], ['fast'], [], ['available'], ['center', 'rough', 'no', 'outside', 'less', 'no', 'unclean'], ['improved', 'more'], ['limited'], ['less'], ['proper', 'available'], [], ['some'], [], [], [], ['sitting', 'less'], [], ['net'], ['clean'], ['clean', 'kept', 'clean', 'sitting', 'proper', 'clean'], ['good'], ['hot', 'clean', 'little'], ['large', 'better'], ['center', 'limited', 'standing', 'standing', 'outside', 'large', 'better'], ['sitting', 'medical', 'more', 'good'], ['dirty', 'changed'], ['less'], ['open', 'good', 'sitting'], ['big'], ['some', 'some'], ['connected', 'no', 'sitting', 'all'], ['clean'], ['clean'], ['comfortable'], ['proper'], ['large'], ['more', 'better', 'small', 'square', 'fit', 'more'], ['short', 'small'], ['clean', 'like'], ['cold', 'cold', 'least', 'cold'], ['different'], ['little', 'informed', 'much'], ['improved', 'much', 'found'], [], ['more', 'proper'], ['larger', 'better'], ['good', 'separate', 'outside', 'clean', 'comfortable'], ['cool'], [], [], ['center', 'sitting'], ['more', 'less'], ['clean'], ['left', 'increased', 'helpful'], ['uncomfortable', 'able', 'small'], ['available'], ['little', 'less', 'small', 'big'], ['separate'], ['different'], ['first'], ['more'], [], ['much', 'no', 'concrete', 'on'], ['clean', 'clean'], ['available', 'center', 'available'], ['big', 'comfortable', 'good', 'like', 'seated'], [], ['more', 'small'], ['on'], ['clean']]\n"
     ]
    }
   ],
   "source": [
    "print tt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_rule = [['clean', 'net', 'cleaning', 'dirty'], ['more', 'large', 'less', 'much', 'small', 'big'], ['sitting', 'standing'], ['different', 'separated', 'separate'],['comfortable']]\n",
    "def space_split_run(adj_list, set_rule):\n",
    "    labels = []\n",
    "    for tmp_set in adj_list:\n",
    "        if len(tmp_set) > 0:\n",
    "            scores = []\n",
    "            for rule_list in set_rule:\n",
    "                scores.append(sim_check_list(tmp_set, rule_list))\n",
    "            if max(scores) > 0.5:\n",
    "                labels.append(scores.index(max(scores)))\n",
    "            else:\n",
    "                labels.append(1)\n",
    "        else:\n",
    "            labels.append(1)\n",
    "    return labels\n",
    "idx_labels = space_split_run(tt_list,set_rule)\n",
    "\n",
    "def write_all(content_list, idx_labels, keyword):\n",
    "    for i in range(-1, max(idx_labels)+2):\n",
    "        f1 = open('cluster/%s/%s_%d.txt' %(keyword, keyword,i),  'w+')\n",
    "        for idx, doc in enumerate(content_list):\n",
    "            if idx_labels[idx] == i:\n",
    "                f1.write('%s\\n' % (content_list[idx]))\n",
    "        f1.close()\n",
    "write_all(local_content, idx_labels, 'space')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 4, -1, 1, -1, 1, 2, 1, 0, 1, 1, 3, -1, 1, 1, 1, 3, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 3, 1, -1, 1, -1, 1, 1, -1, 1, -1, 1, -1, 0, 1, -1, 1, -1, 1, -1, 1, 1, 1, 1, 1, 0, 0, 0, -1, 0, 1, 1, 1, 0, 1, 2, 1, -1, 2, 0, 0, 4, -1, 1, 1, 1, 0, -1, 3, 1, 1, 1, 1, 1, 0, -1, 1, 1, 2, 1, 0, -1, 1, -1, 1, 3, 3, -1, 1, 1, 1, 0, -1, 1, 1, 1, -1, 0]\n"
     ]
    }
   ],
   "source": [
    "print idx_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
