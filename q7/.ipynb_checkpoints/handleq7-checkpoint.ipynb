{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "import itertools\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models.keyedvectors import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_file(n_clusters_, labels, corpus, mapping_dict, test_corpus):\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        # print indice_cluster\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        f1 = open('cluster_%s/%dth_cluster.txt' % (question_class, indice_cluster), 'w+')\n",
    "        for idx in idx_list:\n",
    "            f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "    f1.close()\n",
    "\n",
    "\n",
    "def write_ne(nes, question_class, mapping_dict):\n",
    "    f1 = open('ne_%s.txt' % question_class, 'w+')\n",
    "    for idx, doc in enumerate(nes):\n",
    "        f1.write('%s %s\\n' % (mapping_dict[idx], ' '.join(doc)))\n",
    "    f1.close()    \n",
    "\n",
    "def rule_q7(sen, ne):\n",
    "    clean_ne = list(set(ne))\n",
    "    remove_words = ['customer', 'customers', 'waiting', 'facility', 'facilities', 'dealership', 'toyota']\n",
    "    clean_ne = [word for word in clean_ne if word not in remove_words]\n",
    "\n",
    "    if len(clean_ne) > 1 and 'area' in clean_ne:\n",
    "        clean_ne.remove('area')\n",
    "    if len(clean_ne) < 2 and 'area' in clean_ne:\n",
    "        clean_ne[clean_ne.index('area')] = 'space'\n",
    "    if 'place' in clean_ne:\n",
    "        clean_ne[clean_ne.index('place')] = 'space'\n",
    "    return clean_ne\n",
    "\n",
    "def wmd_compute(x1, x2):\n",
    "    return wv.wmdistance(x1, x2)\n",
    "\n",
    "\n",
    "def clean_corpus(x1):\n",
    "    x_new = []\n",
    "    for x in x1:\n",
    "        if 'no improvement' in x:\n",
    "            continue\n",
    "        else:\n",
    "            x_new.append(x)\n",
    "    return x_new\n",
    "\n",
    "\n",
    "def analyze_cluster(n_clusters_, labels, corpus, mapping_dict, test_corpus, filename):\n",
    "    num = 0\n",
    "    other_content = []\n",
    "    for indice_cluster in range(n_clusters_):\n",
    "        idx_list = np.where(labels == indice_cluster)[0]\n",
    "        if len(idx_list) > 4:\n",
    "            f1 = open('%s_%s/%dth_cluster.txt' % (filename, question_class, indice_cluster), 'w+')\n",
    "            for idx in idx_list:\n",
    "                f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "            f1.close()\n",
    "        if len(idx_list) < 4:\n",
    "            num = num + len(idx_list)\n",
    "            for idx in idx_list:\n",
    "                other_content.append(corpus[mapping_dict[idx] - 1])\n",
    "    f1 = open('%s_%s/other_cluster.txt' % (filename, question_class), 'w+')\n",
    "    for doc in other_content:\n",
    "        f1.write('%s\\n' % doc)\n",
    "    f1.close()\n",
    "    print num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n",
      "452\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b9cabf02af77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mpos_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_new\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tags\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mnn_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mnn_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule_q7\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wv' is not defined"
     ]
    }
   ],
   "source": [
    "question_class = 'dataset//q7'\n",
    "fname = '..//%s.txt' % question_class\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content]\n",
    "f.close()\n",
    "\n",
    "nn_corpus = []\n",
    "\n",
    "# English stop words lists\n",
    "stop_words = stopwords.words('english')\n",
    "punctuation_list = [unicode(i) for i in string.punctuation]\n",
    "for punctuation in punctuation_list:\n",
    "    stop_words.append(punctuation)\n",
    "\n",
    "\n",
    "pos_tags = ['NN', 'NNS']\n",
    "# extract noun\n",
    "mapping_dict = []\n",
    "\n",
    "print len(content)\n",
    "content = clean_corpus(content)\n",
    "print len(content)\n",
    "for review_count, new in enumerate(content):\n",
    "    sents = sent_tokenize(new)\n",
    "    if len(sents) > 1:\n",
    "        continue\n",
    "        \"\"\"\n",
    "        for sen in sents:\n",
    "            nn_list = []\n",
    "            pos_new = nltk.pos_tag(nltk.word_tokenize(sen))\n",
    "            for token in pos_new:\n",
    "                if token[1] in pos_tags and not token[0] in stop_words and len(list(token[0]))>1 and token[0] in wv.vocab:\n",
    "                    nn_list.append(token[0])\n",
    "            #multiple sentence no need\n",
    "            nn_list = rule_q7(sen, nn_list)\n",
    "            if nn_list != []:\n",
    "                nn_corpus.append(nn_list)\n",
    "                mapping_dict.append(review_count+1)\n",
    "        \"\"\"\n",
    "    else:\n",
    "        nn_list = []\n",
    "        sen = sents[0]\n",
    "        pos_new = nltk.pos_tag(nltk.word_tokenize(sen))\n",
    "        for token in pos_new:\n",
    "            if token[1] in pos_tags and not token[0] in stop_words and token[0] in wv.vocab:\n",
    "                nn_list.append(token[0])\n",
    "        nn_list = rule_q7(sen, nn_list)\n",
    "        if nn_list != []:\n",
    "            # print new\n",
    "            nn_corpus.append(nn_list)  # use set or list?\n",
    "            mapping_dict.append(review_count + 1)\n",
    "\n",
    "test_corpus = nn_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-75743ca545fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mab\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_corpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "for ab in test_corpus:\n",
    "    text = text + ab\n",
    "from collections import Counter\n",
    "df = Counter(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'space': 56, 'time': 25, 'tea': 19, 'water': 14, 'arrangement': 12, 'coffee': 11, 'ac': 11, 'tv': 9, 'service': 9, 'room': 8, 'staff': 7, 'car': 7, 'magazine': 5, 'washroom': 4, 'seating': 4, 'vehicle': 4, 'center': 4, 'improvement': 3, 'television': 3, 'servicing': 3, 'chairs': 3, 'toyota': 3, 'owner': 3, 'lot': 3, 'snacks': 3, 'needs': 3, 'driver': 3, 'summer': 2, 'chair': 2, 'food': 2, 'condition': 2, 'sofa': 2, 'canteen': 2, 'hours': 2, 'respondent': 2, 'rush': 2, 'newspaper': 2, 'compare': 2, 'something': 2, 'need': 2, 'cleaning': 2, 'response': 2, 'entertainment': 2, 'increase': 2, 'cabins': 1, 'executive': 1, 'comfortless': 1, 'machine': 1, 'disturbance': 1, 'refreshment': 1, 'location': 1, 'arrange': 1, 'bit': 1, 'cage': 1, 'bas': 1, 'morning': 1, 'queries': 1, 'dustbin': 1, 'seat': 1, 'computer': 1, 'capacity': 1, 'cleanliness': 1, 'particles': 1, 'fans': 1, 'receiving': 1, 'issue': 1, 'noise': 1, 'terms': 1, 'power': 1, 'wi': 1, 'job': 1, 'advisor': 1, 'box': 1, 'entitlement': 1, 'anything': 1, 'launch': 1, 'host': 1, 'games': 1, 'hospitality': 1, 'ladies': 1, 'family': 1, 'ground': 1, 'cofee': 1, 'wifi': 1, 'dust': 1, 'table': 1, 'eminence': 1, 'consuming': 1, 'cleanness': 1, 'sunlight': 1, 'camera': 1, 'way': 1, 'cabin': 1, 'company': 1, 'look': 1, 'places': 1, 'comics': 1, 'work': 1, 'pace': 1, 'dirty': 1, 'toilet': 1, 'counter': 1, 'tap': 1, 'process': 1, 'suggestions': 1, 'foods': 1, 'arrangements': 1, 'breakfast': 1, 'floor': 1, 'drivers': 1, 'comfort': 1, 'lounge': 1, 'waters': 1, 'tries': 1, 'services': 1, 'boor': 1})\n",
      "Counter({'space': 56, 'time': 25, 'tea': 19, 'water': 14, 'arrangement': 12, 'coffee': 11, 'ac': 11, 'tv': 9, 'service': 9, 'room': 8, 'staff': 7, 'car': 7, 'magazine': 5, 'washroom': 4, 'seating': 4, 'vehicle': 4, 'center': 4, 'improvement': 3, 'television': 3, 'servicing': 3, 'chairs': 3, 'toyota': 3, 'owner': 3, 'lot': 3, 'snacks': 3, 'needs': 3, 'driver': 3, 'summer': 2, 'chair': 2, 'food': 2, 'condition': 2, 'sofa': 2, 'canteen': 2, 'hours': 2, 'respondent': 2, 'rush': 2, 'newspaper': 2, 'compare': 2, 'something': 2, 'need': 2, 'cleaning': 2, 'response': 2, 'entertainment': 2, 'increase': 2, 'cabins': 1, 'executive': 1, 'comfortless': 1, 'machine': 1, 'disturbance': 1, 'refreshment': 1, 'location': 1, 'arrange': 1, 'bit': 1, 'cage': 1, 'bas': 1, 'morning': 1, 'queries': 1, 'dustbin': 1, 'seat': 1, 'computer': 1, 'capacity': 1, 'cleanliness': 1, 'particles': 1, 'fans': 1, 'receiving': 1, 'issue': 1, 'noise': 1, 'terms': 1, 'power': 1, 'wi': 1, 'job': 1, 'advisor': 1, 'box': 1, 'entitlement': 1, 'anything': 1, 'launch': 1, 'host': 1, 'games': 1, 'hospitality': 1, 'ladies': 1, 'family': 1, 'ground': 1, 'cofee': 1, 'wifi': 1, 'dust': 1, 'table': 1, 'eminence': 1, 'consuming': 1, 'cleanness': 1, 'sunlight': 1, 'camera': 1, 'way': 1, 'cabin': 1, 'company': 1, 'look': 1, 'places': 1, 'comics': 1, 'work': 1, 'pace': 1, 'dirty': 1, 'toilet': 1, 'counter': 1, 'tap': 1, 'process': 1, 'suggestions': 1, 'foods': 1, 'arrangements': 1, 'breakfast': 1, 'floor': 1, 'drivers': 1, 'comfort': 1, 'lounge': 1, 'waters': 1, 'tries': 1, 'services': 1, 'boor': 1})\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "def filter_ne(test_corpus, df):\n",
    "    for xth, doc in enumerate(test_corpus):\n",
    "        if len(doc)>2:\n",
    "            df_words = [df[word] for word in doc]\n",
    "            idx =  heapq.nlargest(2, xrange(len(df_words)), key=df_words.__getitem__)\n",
    "            test_corpus[xth] = [doc[ith] for ith in idx]\n",
    "    return test_corpus\n",
    "test_corpus = filter_ne(test_corpus, df)\n",
    "write_ne(test_corpus, question_class, mapping_dict)\n",
    "text = []\n",
    "for ab in test_corpus:\n",
    "    text = text + ab\n",
    "from collections import Counter\n",
    "df = Counter(text)\n",
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.58596682549\n",
      "0.0\n",
      "5.13240470791\n",
      "clustering\n",
      "3.58596682549\n",
      "0.0\n",
      "5.13240470791\n",
      "clustering\n",
      "Estimated number of clusters: 20\n",
      "2\n",
      "Estimated number of clusters: 20\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "my_list = range(len(test_corpus))\n",
    "\n",
    "# compute distance matrix\n",
    "dist_matrix = np.zeros((len(test_corpus), len(test_corpus)), dtype=float)\n",
    "\n",
    "for pair in itertools.product(my_list, repeat=2):\n",
    "    dist_matrix[pair[0], pair[1]] = wmd_compute(test_corpus[pair[0]], test_corpus[pair[1]])\n",
    "    if not np.isfinite(dist_matrix[pair[0], pair[1]]):\n",
    "        print pair[0]\n",
    "        print pair[1]\n",
    "        print 'tt'\n",
    "\n",
    "\n",
    "similarity_matrix = np.negative(dist_matrix) + np.amax(dist_matrix)\n",
    "\n",
    "print np.median(dist_matrix)\n",
    "\n",
    "print np.min(dist_matrix)\n",
    "print np.max(dist_matrix)\n",
    "value = (np.median(similarity_matrix) + np.min(similarity_matrix)) / 2\n",
    "####\n",
    "\n",
    "# print dist_matrix\n",
    "# print wmd_compute(nn_corpus[0], nn_corpus[1])\n",
    "#tt = pairwise_distances(nn_corpus, metric=wmd_compute)\n",
    "\n",
    "print 'clustering'  # clustering algorithm\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "nscores = []\n",
    "ncluster_range = [6, 8, 10, 12, 14, 16, 18, 20]\n",
    "for num_cluster in ncluster_range:\n",
    "    y_predict = KMeans(init='k-means++', n_clusters=num_cluster, n_init=10, max_iter=1000, precompute_distances=True).fit_predict(dist_matrix)\n",
    "    nscores.append(silhouette_score(dist_matrix, y_predict, metric='precomputed'))\n",
    "\n",
    "    \n",
    "idx_max = np.argmax(np.array(nscores))\n",
    "n_clusters_ = ncluster_range[idx_max]\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "\n",
    "y_predict = KMeans(init='k-means++', n_clusters=n_clusters_, n_init=10, max_iter=1000, precompute_distances=True).fit_predict(dist_matrix)\n",
    "analyze_cluster(n_clusters_, y_predict, content, mapping_dict, test_corpus, 'clusterv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-b9e375226d08>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-b9e375226d08>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    if wn.synsets(word)[0].pos() == 'a':\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-28-b9e375226d08>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-b9e375226d08>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    if wn.synsets(word)[0].pos() == 'a':\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "    \n",
    "    \n",
    "def analyze_someclass(keyword, corpus, mapping_dict, test_corpus):\n",
    "    num = 0\n",
    "    other_content = []\n",
    "    f1 = open('%s_docs.txt', 'w+')\n",
    "    for idx, doc in enumerate(test_corpus):\n",
    "        if keyword in doc:\n",
    "            adj_list = []\n",
    "            for word in corpus[mapping_dict[idx]:\n",
    "                tmp = wn.synsets(word)[0].pos()\n",
    "                if tmp == 'a':\n",
    "                    adj_list.append(word)\n",
    "            if len(adj_list)>0:\n",
    "                adjwords = ' '.join(adj_list)\n",
    "                f1.write('%s %s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx], adjwords))\n",
    "            else:\n",
    "                f1.write('%s %s\\n' % (corpus[mapping_dict[idx] - 1], test_corpus[idx]))\n",
    "    f1.close()\n",
    "analyze_someclass('space', content, mapping_dict, test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
